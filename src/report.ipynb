{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3633b89",
   "metadata": {},
   "source": [
    "# Basics of Mobile Robotics, Project report \n",
    "\n",
    "The final project for EPFL's [**MICRO-452: Basics of Mobile Robotics**](https://edu.epfl.ch/coursebook/en/basics-of-mobile-robotics-MICRO-452) course. This project implements a complete navigation stack for a differential drive robot, featuring computer vision, graph-based path planning, sensor fusion (EKF), and reactive control.\n",
    "\n",
    "**The Concept:** Inspired by the Disney movie WALL¬∑E, the robot EVE must navigate through the interior of the spaceship Axiom - a complex and hazardous environment. Her mission is to reach, her love,  WALL¬∑E as fast as possible, while avoiding a series of dangerous obstacles.\n",
    "The map includes:\n",
    "\n",
    "- Windows: Represented as static dark-blue polygonal regions that EVE must not cross. These areas act as boundaries or fatal zones, similar to voids in the spacecraft‚Äôs structure.\n",
    "\n",
    "- Humans (‚ÄúPassengers‚Äù): Large, drifting individuals who move unpredictably through the corridors. They function as dynamic obstacles that EVE must detect and avoid in real time.\n",
    "\n",
    "![Example environment](../vision_debug/warped.jpg)\n",
    "\n",
    "\n",
    "## Group members & tasks repartition\n",
    "- Tancrede Lamort De Gail: Vision, global navigation, motion control\n",
    "- John Constantin: Vision, global navigation, motion control\n",
    "- Marcus Edjolo: Local naviation, global navigation, motin control\n",
    "- Yvan Barragan: Filtering, motion control\n",
    "\n",
    "\n",
    "## 1. Physical Setup\n",
    "\n",
    "The project tasks a Thymio II robot with navigating from a start pose to a goal pose while avoiding two distinct types of obstacles.\n",
    "\n",
    "### 1.1 The Environment\n",
    "\n",
    "The playground consist of a white rectangle of 130x92 cm delimitated by four Aruco Marker place on each of the 4 corners.\n",
    "\n",
    "The environment presents a dual-layer challenge:\n",
    "\n",
    "1.  **Static \"Global\" Obstacles (The Windows):**\n",
    "    -   **Physicality:** Flat, dark blue polygonal cutouts.\n",
    "    -   **Visibility:** _Invisible_ to the robot's onboard horizontal proximity sensors, but clearly visible to the overhead global camera.\n",
    "    -   **Constraint:** The robot cannot drive over them. Avoiding them relies entirely on accurate global localization and path planning.\n",
    "2.  **Ephemeral \"Local\" Obstacles (The Passengers):**\n",
    "    -   **Physicality:** 3D physical objects (cylinders, blocks) roughly the size of the robot.\n",
    "    -   **Visibility:** Visible to the robot's onboard sensors, but _ignored_ by the global camera mapping (or placed after mapping is complete).\n",
    "    -   **Constraint:** These act as dynamic blockers. The robot must use local sensing to reactively avoid them without losing track of its global objective.\n",
    "\n",
    "### 1.2 The Robot\n",
    "\n",
    "The **Thymio II** is a differential-drive mobile robot.\n",
    "\n",
    "-   **Sensors:** 5 front-facing and 2 rear-facing horizontal IR proximity sensors (for local avoidance).\n",
    "-   **Comms:** Python API interfacing via USB/RF dongle.\n",
    "-   **Markers:** An ArUco marker is mounted on top of the robot to facilitate high-precision global tracking (Pose: $x, y, \\theta$) via the overhead camera.\n",
    "\n",
    "### 1.3 The Camera\n",
    "\n",
    "We're usging our smartphone mounted overhead to have a better quality and beging less sentistive to the environment. With the software \"Camo studio\" we are able the read the phone camera at $1920 \\times 1080$, RGB, 30 FPS and transmit to the program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3167dcd",
   "metadata": {},
   "source": [
    "## 2. Vision System\n",
    "The vision systeme serves as the \"GPS\" of the system, responsible for:\n",
    "\n",
    "1.  **Initial Mapping:** Detecting static obstacle polygons and the goal area.\n",
    "2.  **Live Tracking:** Providing absolute position estimates to correct the robot's drifting odometry.\n",
    "\n",
    "\n",
    "### 2.1 System Architecture\n",
    "\n",
    "The vision pipeline operates in two distinct phases:\n",
    "\n",
    "#### 2.1.1 Initialization Phase (Static Mapping)\n",
    "During startup, the system captures a single reference frame and extracts the complete static environment:\n",
    "- Calibration markers define the workspace boundaries\n",
    "- Static obstacles (the \"spacecraft windows\") are segmented and converted to polygonal representations\n",
    "- Goal location (WALL¬∑E's position) is identified\n",
    "- Visibility graph is pre-computed for path planning\n",
    "\n",
    "#### 2.1.2 Runtime Phase (Real-Time Localization)\n",
    "During navigation, the system continuously:\n",
    "- Captures frames at approximately 10 Hz\n",
    "- Detects (EVE's position) the robot's ArUco marker\n",
    "- Computes the robot's pose $(x, y, \\theta)$\n",
    "- Provides measurements to the Extended Kalman Filter for sensor fusion\n",
    "\n",
    "### 2.2 Coordinate Systems and Perspective Transformation\n",
    "\n",
    "The vision system must bridge two distinct coordinate frames:\n",
    "\n",
    "**Camera Frame:** The raw image captured by the overhead camera, measured in pixels with origin at top-left, where coordinates are $(u, v) \\in [0, 1920] \\times [0, 1080]$.\n",
    "\n",
    "**World Frame:** The robot's navigation space, measured in centimeters with origin at bottom-left of the workspace, where coordinates are $(x, y) \\in [0,132.5 ] \\times [0, 92.5]$.\n",
    "\n",
    "#### 2.2.1 Perspective Correction\n",
    "\n",
    "Due to the camera's mounting angle and lens distortion, the raw image exhibits perspective distortion. We use a **homography transformation** to warp the image into an orthographic (bird's-eye) view:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} u' \\\\ v' \\\\ w \\end{bmatrix} = \\mathbf{H} \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{H} \\in \\mathbb{R}^{3 \\times 3}$ is the perspective transformation matrix computed via OpenCV's `getPerspectiveTransform()` using the four corner calibration markers as control points.\n",
    "\n",
    "The final pixel coordinates after perspective correction are:\n",
    "$$\n",
    "x_{\\text{px}} = \\frac{u'}{w}, \\quad y_{\\text{px}} = \\frac{v'}{w}\n",
    "$$\n",
    "\n",
    "#### 2.2.2 Metric Conversion\n",
    "\n",
    "After warping, we convert from the rectified pixel space to world coordinates (cm):\n",
    "\n",
    "$$\n",
    "x_{\\text{cm}} = \\frac{x_{\\text{px}}}{\\alpha_x}, \\quad y_{\\text{cm}} = H_{\\text{map}} - \\frac{y_{\\text{px}}}{\\alpha_y}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha_x = \\frac{W_{\\text{px}}}{W_{\\text{cm}}}$ and $\\alpha_y = \\frac{H_{\\text{px}}}{H_{\\text{cm}}}$ are pixel-to-centimeter scaling factors\n",
    "- $H_{\\text{map}}$ is the map height (flipping y-axis from image convention)\n",
    "\n",
    "\n",
    "### 2.3 Environment Segmentation\n",
    "\n",
    "#### 2.3.1 Color-Based Segmentation\n",
    "We segment obstacles and the goal using **HSV color space** thresholding (finding experimentally), which is more robust to lighting variations than RGB:\n",
    "\n",
    "**Static Obstacles (Dark Blue):**\n",
    "$$\n",
    "\\text{Mask}_{\\text{obstacle}} = \\{(h,s,v) \\mid 90¬∞ \\leq h \\leq 165¬∞, \\, 40 \\leq s \\leq 255, \\, 80 \\leq v \\leq 255\\}\n",
    "$$\n",
    "\n",
    "**Goal Region (Red):**\n",
    "Due to red's wraparound in HSV (0¬∞ and 360¬∞ are both red), we use a dual-range mask:\n",
    "$$\n",
    "\\text{Mask}_{\\text{goal}} = \\text{Mask}_{\\text{red1}} \\cup \\text{Mask}_{\\text{red2}}\n",
    "$$\n",
    "where:\n",
    "- $\\text{Mask}_{\\text{red1}}: 0¬∞ \\leq h \\leq 15¬∞$\n",
    "- $\\text{Mask}_{\\text{red2}}: 165¬∞ \\leq h \\leq 180¬∞$\n",
    "\n",
    "\n",
    "#### 2.3.2 Morphological Filtering\n",
    "To eliminate noise and fill gaps in the segmented regions, we apply morphological operations with a $7 \\times 7$ kernel:\n",
    "1. **Opening** (erosion ‚Üí dilation): Removes small noise blobs\n",
    "2. **Closing** (dilation ‚Üí erosion): Fills internal holes in obstacles\n",
    "\n",
    "<div style=\"display: flex; gap: 20px; justify-content: center; align-items: flex-start;\">\n",
    "\n",
    "  <figure style=\"margin: 0; text-align: center;\">\n",
    "    <img src=\"../vision_debug/obstacle_mask.jpg\" alt=\"Obstacle mask\" width=\"300\"/>\n",
    "    <figcaption>Obstacle mask</figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"margin: 0; text-align: center;\">\n",
    "    <img src=\"../vision_debug/target_mask.jpg\" alt=\"Target mask\" width=\"300\"/>\n",
    "    <figcaption>Target mask</figcaption>\n",
    "  </figure>\n",
    "</div>\n",
    "\n",
    "\n",
    "### 2.4 Configuration Space Construction\n",
    "\n",
    "#### 2.4.1 Polygon Approximation\n",
    "Raw contours from segmentation are noisy and contain hundreds of vertices. We simplify them using the **Douglas-Peucker algorithm**:\n",
    "\n",
    "$$\n",
    "\\epsilon = 0.015 \\cdot \\text{arcLength}(\\text{contour})\n",
    "$$\n",
    "\n",
    "This produces polygons with 4-12 vertices while preserving essential shape features.\n",
    "\n",
    "#### 2.4.2 C-Space Inflation\n",
    "To account for the robot's physical dimensions, we **buffer** each obstacle polygon by the robot's radius plus a safety margin:\n",
    "\n",
    "$$\n",
    "\\mathcal{O}_{\\text{buffered}} = \\mathcal{O} \\oplus B_r\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{O}$ is the original obstacle polygon\n",
    "- $B_r$ is a disk of radius $r = r_{\\text{robot}} + \\text{padding}$ (‚âà 5.5 cm)\n",
    "- $\\oplus$ denotes Minkowski sum (implemented via Shapely's `buffer()` with mitre join style)\n",
    "\n",
    "This operation effectively reduces the robot to a point in configuration space, simplifying collision checking.\n",
    "\n",
    "#### 2.4.3 Vertex Merging\n",
    "The buffering operation can introduce redundant vertices on long straight edges. To reduce graph complexity, we merge vertices closer than a threshold $d_{\\text{min}} = 6.0$ cm:\n",
    "\n",
    "$$\n",
    "\\text{if } \\|v_i - v_{i+1}\\| < d_{\\text{min}}, \\quad v_{\\text{merged}} = \\frac{v_i + v_{i+1}}{2}\n",
    "$$\n",
    "\n",
    "### 2.5 Configuration Space Construction\n",
    "\n",
    "#### 2.5.1 Polygon Approximation\n",
    "Raw contours from segmentation are noisy and contain hundreds of vertices. We simplify them using the **Douglas-Peucker algorithm**:\n",
    "\n",
    "$$\n",
    "\\epsilon = 0.015 \\cdot \\text{arcLength}(\\text{contour})\n",
    "$$\n",
    "\n",
    "This produces polygons with 4-12 vertices while preserving essential shape features.\n",
    "\n",
    "#### 2.5.2 C-Space Inflation\n",
    "To account for the robot's physical dimensions, we **buffer** each obstacle polygon by the robot's radius plus a safety margin:\n",
    "\n",
    "$$\n",
    "\\mathcal{O}_{\\text{buffered}} = \\mathcal{O} \\oplus B_r\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{O}$ is the original obstacle polygon\n",
    "- $B_r$ is a disk of radius $r = r_{\\text{robot}} + \\text{padding}$ (‚âà 5.5 cm)\n",
    "- $\\oplus$ denotes Minkowski sum (implemented via Shapely's `buffer()` with mitre join style)\n",
    "\n",
    "This operation effectively reduces the robot to a point in configuration space, simplifying collision checking.\n",
    "\n",
    "#### 2.5.3 Vertex Merging\n",
    "The buffering operation can introduce redundant vertices on long straight edges. To reduce graph complexity, we merge vertices closer than a threshold $d_{\\text{min}} = 6.0$ cm:\n",
    "\n",
    "$$\n",
    "\\text{if } \\|v_i - v_{i+1}\\| < d_{\\text{min}}, \\quad v_{\\text{merged}} = \\frac{v_i + v_{i+1}}{2}\n",
    "$$\n",
    "\n",
    "### 2.6 Visibility Graph Construction\n",
    "\n",
    "The visibility graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ encodes all collision-free straight-line paths through the environment. We leverage two Python libraries for this task: **Shapely** for geometric operations (polygon intersection, line-polygon collision detection) and **NetworkX** for graph representation and manipulation.\n",
    "\n",
    "#### 2.6.1 Node Addition\n",
    "We begin by constructing the node set $\\mathcal{V}$, which includes all vertices from the buffered obstacle polygons plus the goal location:\n",
    "\n",
    "$$\n",
    "\\mathcal{V} = \\mathcal{V}_{\\text{obstacles}} \\cup \\{v_{\\text{goal}}\\}\n",
    "$$\n",
    "\n",
    "Each node $v_i$ is added to a NetworkX graph with an attribute `pos` storing its $(x, y)$ coordinates in centimeters.\n",
    "\n",
    "#### 2.6.2 Obstacle Edge Insertion\n",
    "Before computing visibility between arbitrary node pairs, we first add all **obstacle boundary edges** to the graph. For each obstacle polygon with vertices $\\{v_0, v_1, \\ldots, v_{n-1}\\}$, we connect consecutive vertices:\n",
    "\n",
    "_________________________________________________________\n",
    "$$\n",
    "\\mathcal{E}_{\\text{obstacle}} = \\{(v_i, v_{i+1 \\mod n}) \\mid i = 0, \\ldots, n-1\\}\n",
    "$$\n",
    "\n",
    "Each edge is weighted by Euclidean distance:\n",
    "$$\n",
    "w(v_i, v_j) = \\|v_i - v_j\\|_2 = \\sqrt{(x_j - x_i)^2 + (y_j - y_i)^2}\n",
    "$$\n",
    "\n",
    "This ensures that paths can traverse obstacle perimeters when necessary.\n",
    "_____________________________________________________________\n",
    "#### 2.6.3 Visibility Edge Computation\n",
    "Next, we test visibility between all pairs of nodes. Two nodes $v_i, v_j$ are connected by an edge if:\n",
    "1. They are not already connected (to avoid duplicate obstacle edges)\n",
    "2. The line segment $\\overline{v_i v_j}$ does not intersect any obstacle interior\n",
    "\n",
    "We implement visibility checking using Shapely's geometric predicates. The test verifies that the line segment neither crosses through nor lies entirely within any obstacle:\n",
    "\n",
    "$$\n",
    "\\text{visible}(v_i, v_j) = \\begin{cases}\n",
    "\\text{True} & \\text{if } \\overline{v_i v_j} \\cap \\mathcal{O}_k = \\emptyset \\, \\forall k \\text{ and } \\overline{v_i v_j} \\not\\subset \\mathcal{O}_k \\, \\forall k \\\\\n",
    "\\text{False} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The second condition is critical for concave obstacles - a line segment could lie entirely within a U-shaped obstacle without technically \"intersecting\" its boundary.\n",
    "\n",
    "**Shrinking Heuristic:** To avoid false positives from floating-point precision issues at polygon vertices, we test a **slightly shortened** version of each line segment:\n",
    "\n",
    "$$\n",
    "\\overline{v_i v_j}_{\\text{test}} = \\{v_i + 0.001 \\cdot (v_j - v_i) \\to v_i + 0.999 \\cdot (v_j - v_i)\\}\n",
    "$$\n",
    "\n",
    "This prevents spurious intersections when paths graze obstacle corners.\n",
    "\n",
    "#### 2.6.4 Boundary Cropping\n",
    "After constructing the full graph, we perform a final validation step: **remove any nodes that lie outside the map boundaries**. Due to the buffering operation, some obstacle vertices may extend beyond the physical workspace limits $[0, W_{\\text{map}}] \\times [0, H_{\\text{map}}]$.\n",
    "\n",
    "We identify and remove all such nodes:\n",
    "$$\n",
    "\\mathcal{V}_{\\text{invalid}} = \\{v_i \\in \\mathcal{V} \\mid x_i < 0 \\text{ or } x_i > W_{\\text{map}} \\text{ or } y_i < 0 \\text{ or } y_i > H_{\\text{map}}\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{V}_{\\text{final}} = \\mathcal{V} \\setminus \\mathcal{V}_{\\text{invalid}}\n",
    "$$\n",
    "\n",
    "NetworkX automatically removes all edges connected to deleted nodes, ensuring graph integrity.\n",
    "\n",
    "#### 2.6.5 Base Graph Storage\n",
    "The result of this process is the **base visibility graph**, which contains only obstacle vertices and the goal node. This graph is computed once during initialization and stored as `self.base_graph`. The robot's start position is intentionally excluded at this stage to enable dynamic replanning after kidnapping events.\n",
    "\n",
    "### 2.7 Dynamic Graph Augmentation\n",
    "\n",
    "Since the robot's start position changes after each kidnapping event, the visibility graph must be **dynamically updated**:\n",
    "\n",
    "1. **Base Graph:** Contains only obstacle vertices and the goal (precomputed once)\n",
    "2. **Runtime Augmentation:** When the robot is relocated:\n",
    "   - Add a new start node $v_{\\text{start}}$ at the robot's current pose\n",
    "   - Test visibility from $v_{\\text{start}}$ to all existing nodes\n",
    "   - Connect $v_{\\text{start}}$ to all visible nodes with weighted edges\n",
    "   - Run A* search on the augmented graph\n",
    "\n",
    "This strategy avoids recomputing the entire $O(n^3)$ visibility graph (where $n \\approx 40$ nodes) after every kidnapping - we only perform $O(n^2)$ visibility tests from the new start node.\n",
    "\n",
    "### 2.8 ArUco Marker Detection and Pose Estimation\n",
    "\n",
    "#### 2.8.1 Pose Computation\n",
    "Given the four corners of the detected marker $\\{C_0, C_1, C_2, C_3\\}$ (ordered as Top-Left, Top-Right, Bottom-Right, Bottom-Left), we compute:\n",
    "\n",
    "**Position** (centroid):\n",
    "$$\n",
    "x = \\frac{1}{4}\\sum_{i=0}^{3} C_{i,x}, \\quad y = \\frac{1}{4}\\sum_{i=0}^{3} C_{i,y}\n",
    "$$\n",
    "\n",
    "**Orientation** (from marker's left edge):\n",
    "$$\n",
    "\\theta = \\text{atan2}(C_{0,y} - C_{3,y}, \\, C_{0,x} - C_{3,x})\n",
    "$$\n",
    "\n",
    "This orientation vector points along the robot's forward direction, as the marker is mounted with its left edge aligned with the robot's heading.\n",
    "\n",
    "### 2.8 Measurement Uncertainty\n",
    "\n",
    "The camera provides pose estimates $(x, y, \\theta)$ with empirically measured covariance:\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\text{camera}} = \\begin{bmatrix}\n",
    "0.00119 & 0 & 0 \\\\\n",
    "0 & 0.00179 & 0 \\\\\n",
    "0 & 0 & 0.000061\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This translates to standard deviations of approximately:\n",
    "- Position: $\\sigma_x \\approx 3.4$ mm, $\\sigma_y \\approx 4.2$ mm\n",
    "- Orientation: $\\sigma_\\theta \\approx 0.45¬∞$\n",
    "\n",
    "These values were obtained through repeated measurements of a stationary robot and used to tune the Extended Kalman Filter.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829cbd5",
   "metadata": {},
   "source": [
    "# 3. Path Planning\n",
    "\n",
    "Once the vision system has constructed a visibility graph of the environment, the robot needs to compute the shortest collision-free path from its current position to the goal. We implement this using the **A\\* (A-star) search algorithm**.\n",
    "\n",
    "## 3.1 The A\\* Algorithm\n",
    "\n",
    "A\\* is an informed search algorithm that explores the graph by expanding nodes with the lowest estimated total cost. For each node $n$, it maintains:\n",
    "\n",
    "$$\n",
    "f(n) = g(n) + h(n)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $g(n)$: **Actual cost** from the start node to $n$ (sum of edge weights along the path)\n",
    "- $h(n)$: **Heuristic estimate** of cost from $n$ to the goal\n",
    "- $f(n)$: **Estimated total cost** of the path through $n$\n",
    "\n",
    "### 3.1.1 Admissibility and Optimality\n",
    "\n",
    "For A\\* to guarantee finding the shortest path, the heuristic must be **admissible** (never overestimate the true cost). We use **Euclidean distance** as our heuristic:\n",
    "\n",
    "$$\n",
    "h(n) = \\|p_n - p_{\\text{goal}}\\|_2 = \\sqrt{(x_{\\text{goal}} - x_n)^2 + (y_{\\text{goal}} - y_n)^2}\n",
    "$$\n",
    "\n",
    "This is admissible because straight-line distance is always less than or equal to any path through obstacles. Additionally, since all edge weights in our graph are Euclidean distances (a consistent metric), A\\* will expand each node at most once.\n",
    "\n",
    "### 3.1.2 Algorithm Structure\n",
    "\n",
    "The implementation uses a **priority queue** (min-heap) to efficiently retrieve the node with the lowest $f$-cost at each iteration:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Add start node to the open set with $f(start) = h(start)$\n",
    "   - Set $g(start) = 0$ and $g(n) = \\infty$ for all other nodes\n",
    "\n",
    "2. **Main Loop:**\n",
    "   ```\n",
    "   while open_set is not empty:\n",
    "       current ‚Üê node with lowest f(current)\n",
    "       \n",
    "       if current == goal:\n",
    "           return reconstruct_path()\n",
    "       \n",
    "       for each neighbor of current:\n",
    "           tentative_g = g(current) + weight(current, neighbor)\n",
    "           \n",
    "           if tentative_g < g(neighbor):\n",
    "               parent[neighbor] = current\n",
    "               g(neighbor) = tentative_g\n",
    "               f(neighbor) = g(neighbor) + h(neighbor)\n",
    "               add neighbor to open_set\n",
    "   ```\n",
    "\n",
    "3. **Path Reconstruction:**\n",
    "   Once the goal is reached, backtrack through the `parent` pointers from goal to start, then reverse the list to obtain the path in start‚Üígoal order.\n",
    "\n",
    "### 3.1.3 Implementation Details\n",
    "\n",
    "**Lazy Deletion:** Python's `heapq` module does not support priority updates. Instead, when we find a better path to a node already in the queue, we simply add a new entry with the improved $f$-cost. The old entry remains in the heap but will be ignored when popped (since its $g$-cost will be worse than the recorded value).\n",
    "\n",
    "**NetworkX Integration:** Our visibility graph is stored as a `networkx.Graph` object where:\n",
    "- Nodes have attribute `pos: Point` (world coordinates)\n",
    "- Edges have attribute `weight: float` (Euclidean distance)\n",
    "\n",
    "This abstraction allows A\\* to operate independently of the graph construction method.\n",
    "\n",
    "## 3.2 Waypoint Generation\n",
    "\n",
    "The output of A\\* is a sequence of node indices:\n",
    "$$\n",
    "\\text{path} = [n_{\\text{start}}, n_1, n_2, \\ldots, n_{k-1}, n_{\\text{goal}}]\n",
    "$$\n",
    "\n",
    "We convert these to world-space waypoints by extracting the `pos` attribute:\n",
    "$$\n",
    "\\text{waypoints} = [(x_0, y_0), (x_1, y_1), \\ldots, (x_k, y_k)]\n",
    "$$\n",
    "\n",
    "The controller then navigates sequentially through these waypoints, moving to the next one once within a threshold distance (3 cm) of the current target.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b618c5",
   "metadata": {},
   "source": [
    "# 4. Local Navigation and Control\n",
    "\n",
    "While the vision system and path planner provide a global roadmap, the robot must execute this plan in the real world using onboard sensors and actuators. This layer handles two critical tasks: (1) **waypoint tracking** - following the planned path, and (2) **reactive obstacle avoidance** - responding to dynamic obstacles not captured during mapping.\n",
    "\n",
    "## 4.1 Control Architecture\n",
    "\n",
    "The control system operates as a **hierarchical state machine** with three states:\n",
    "\n",
    "### 4.1.1 State: NAVIGATING\n",
    "**Objective:** Drive toward the current waypoint.\n",
    "\n",
    "**Control Law:** Proportional control on heading error with speed regulation based on alignment:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_{\\text{error}} &= \\text{normalize}\\left(\\text{atan2}(y_{\\text{target}} - y, x_{\\text{target}} - x) - \\theta\\right) \\\\\n",
    "\\omega &= K_p \\cdot \\theta_{\\text{error}} \\\\\n",
    "v &= v_{\\max} \\cdot \\max(0, \\cos(\\theta_{\\text{error}}))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $K_p = 120$ is the proportional gain for angular velocity\n",
    "- $v_{\\max} = 125$ Thymio units (‚âà 14 cm/s)\n",
    "- $\\text{normalize}(\\cdot)$ wraps angles to $[-\\pi, \\pi]$\n",
    "\n",
    "The cosine term elegantly handles the speed-accuracy tradeoff: when the robot is well-aligned with the target ($\\theta_{\\text{error}} \\approx 0$), it drives at full speed. When misaligned ($|\\theta_{\\text{error}}| \\approx \\pi/2$), it stops forward motion and rotates in place.\n",
    "\n",
    "**Differential Drive Kinematics:**\n",
    "$$\n",
    "\\begin{align}\n",
    "v_L &= v - \\omega \\\\\n",
    "v_R &= v + \\omega\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### 4.1.2 State: AVOIDING\n",
    "**Trigger:** Any front proximity sensor exceeds threshold (400 units ‚âà 4 cm).\n",
    "\n",
    "**Objective:** Steer away from obstacle using a Braitenberg-inspired reactive controller.\n",
    "\n",
    "**Control Law:**\n",
    "$$\n",
    "\\tau = w_{\\text{outer}}(s_0 - s_4) + w_{\\text{inner}}(s_1 - s_3)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $s_i$ are the five front proximity sensor readings (indexed left to right)\n",
    "- $w_{\\text{outer}} = 1.0$, $w_{\\text{inner}} = 1.7$ are tuned weights\n",
    "- $\\tau$ is the turn bias (positive = turn right, negative = turn left)\n",
    "\n",
    "The motor commands become:\n",
    "$$\n",
    "\\begin{align}\n",
    "v_L &= v_{\\text{avoid}} + k \\cdot \\tau \\\\\n",
    "v_R &= v_{\\text{avoid}} - k \\cdot \\tau\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with gain $k = 0.025$.\n",
    "\n",
    "**Adaptive Weighting:** If the left-right sensor difference is too small ($|s_1 - s_3| < 30$), indicating a symmetric frontal obstacle, we disable the inner sensors and increase outer weights to $w_{\\text{outer}} = 2.3$ to encourage a sharper turn.\n",
    "\n",
    "### 4.1.3 State: RETURN (Decay Phase)\n",
    "**Trigger:** Obstacle clears while in AVOIDING state.\n",
    "\n",
    "**Objective:** Smoothly transition back to waypoint tracking without abrupt commands.\n",
    "\n",
    "For duration $T = 4.0$ seconds after obstacle clearance, we **blend** the \"drive straight\" behavior with the waypoint navigation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha(t) &= \\max\\left(0, 1 - \\frac{t}{T}\\right) \\quad \\text{(decay factor)} \\\\\n",
    "v_L &= \\alpha \\cdot v_{\\max} + (1 - \\alpha) \\cdot v_{L,\\text{nav}} \\\\\n",
    "v_R &= \\alpha \\cdot v_{\\max} + (1 - \\alpha) \\cdot v_{R,\\text{nav}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $v_{L/R,\\text{nav}}$ are the outputs from the waypoint tracking controller. This linear interpolation prevents oscillations and ensures the robot smoothly reorients toward the path.\n",
    "\n",
    "## 4.2 Waypoint Advancement\n",
    "\n",
    "The robot monitors its distance to the current waypoint:\n",
    "$$\n",
    "d = \\sqrt{(x_{\\text{waypoint}} - x)^2 + (y_{\\text{waypoint}} - y)^2}\n",
    "$$\n",
    "\n",
    "When $d < 3.0$ cm (approximately the robot's radius), the waypoint is considered reached and the controller advances to the next one in the sequence.\n",
    "\n",
    "## 4.3 Design Rationale\n",
    "\n",
    "**Why State Machine?** Finite state machines provide predictable, debuggable behavior. Each state has a clear purpose and explicit transition conditions.\n",
    "\n",
    "**Why Decay Phase?** Instantaneous switching from obstacle avoidance back to navigation causes \"chattering\" - the robot oscillates between states when near an obstacle. The decay phase acts as a low-pass filter on the control commands.\n",
    "\n",
    "**Why Braitenberg?** While not globally optimal, Braitenberg controllers are computationally trivial (no planning required) and provide robust local reflexes. They complement global planning by handling unforeseen obstacles.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. State Estimation: Extended Kalman Filter\n",
    "\n",
    "The robot has two sources of position information: (1) **odometry** from wheel encoders (high-frequency, but drifts over time), and (2) **camera measurements** from the overhead vision system (accurate but lower frequency and occasionally unavailable). An **Extended Kalman Filter (EKF)** fuses these complementary sources into a single, optimal pose estimate.\n",
    "\n",
    "## 5.1 The State Vector\n",
    "\n",
    "The robot's state is represented as:\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} x \\\\ y \\\\ \\theta \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $(x, y)$ is the position in centimeters and $\\theta$ is the heading in radians. The EKF maintains:\n",
    "- $\\mathbf{x}$: Current state estimate\n",
    "- $\\mathbf{P} \\in \\mathbb{R}^{3 \\times 3}$: State covariance matrix (uncertainty)\n",
    "\n",
    "## 5.2 Motion Model (Prediction Step)\n",
    "\n",
    "The robot's motion follows the standard **unicycle model** for differential drive robots:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dot{x} &= \\frac{v_L + v_R}{2} \\cos(\\theta) \\\\\n",
    "\\dot{y} &= \\frac{v_L + v_R}{2} \\sin(\\theta) \\\\\n",
    "\\dot{\\theta} &= \\frac{v_R - v_L}{L}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $L = 9.5$ cm is the wheelbase and $v_{L/R}$ are the left/right wheel velocities measured from motor encoder feedback.\n",
    "\n",
    "### 5.2.1 Discrete-Time Propagation\n",
    "\n",
    "At each timestep $\\Delta t \\approx 0.1$ s, we propagate the state using Euler integration:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{k+1} = f(\\mathbf{x}_k, \\mathbf{u}_k) = \\begin{bmatrix}\n",
    "x_k + v \\cos(\\theta_k) \\Delta t \\\\\n",
    "y_k + v \\sin(\\theta_k) \\Delta t \\\\\n",
    "\\theta_k + \\omega \\Delta t\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $v = (v_L + v_R)/2$ and $\\omega = (v_R - v_L)/L$.\n",
    "\n",
    "### 5.2.2 Jacobian Computation\n",
    "\n",
    "Since the motion model is nonlinear (due to trigonometric functions), we linearize it around the current state estimate:\n",
    "\n",
    "$$\n",
    "\\mathbf{F}_k = \\frac{\\partial f}{\\partial \\mathbf{x}}\\bigg|_{\\mathbf{x}_k} = \\begin{bmatrix}\n",
    "1 & 0 & -v \\sin(\\theta_k) \\Delta t \\\\\n",
    "0 & 1 & v \\cos(\\theta_k) \\Delta t \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### 5.2.3 Process Noise\n",
    "\n",
    "Wheel slip, uneven surfaces, and encoder quantization introduce uncertainty in the motion. We model this as additive Gaussian noise $\\mathbf{w}_k \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q}_k)$ where the process noise covariance is computed dynamically based on wheel speeds (higher speeds ‚Üí higher uncertainty).\n",
    "\n",
    "**Prediction equations:**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{x}}_{k+1}^{-} &= f(\\hat{\\mathbf{x}}_k, \\mathbf{u}_k) \\\\\n",
    "\\mathbf{P}_{k+1}^{-} &= \\mathbf{F}_k \\mathbf{P}_k \\mathbf{F}_k^T + \\mathbf{Q}_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the superscript $(-)$ denotes the *a priori* estimate before incorporating measurements.\n",
    "\n",
    "## 5.3 Measurement Model (Update Step)\n",
    "\n",
    "The overhead camera directly observes all three state variables, giving the measurement equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_k = \\mathbf{H} \\mathbf{x}_k + \\mathbf{v}_k\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{H} = \\mathbf{I}_{3 \\times 3}$ (identity matrix, since we measure $x$, $y$, $\\theta$ directly)\n",
    "- $\\mathbf{v}_k \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R})$ is measurement noise with covariance:\n",
    "\n",
    "$$\n",
    "\\mathbf{R} = \\begin{bmatrix}\n",
    "0.00119 & 0 & 0 \\\\\n",
    "0 & 0.00179 & 0 \\\\\n",
    "0 & 0 & 0.000061\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This covariance was empirically determined (see Section 2.8).\n",
    "\n",
    "### 5.3.1 Kalman Gain\n",
    "\n",
    "The Kalman gain determines how much to trust the measurement vs. the prediction:\n",
    "\n",
    "$$\n",
    "\\mathbf{K}_k = \\mathbf{P}_k^{-} \\mathbf{H}^T \\left(\\mathbf{H} \\mathbf{P}_k^{-} \\mathbf{H}^T + \\mathbf{R}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "When $\\mathbf{P}_k^{-}$ is large (high prediction uncertainty), $\\mathbf{K}_k$ is large ‚Üí trust measurement more.\n",
    "When $\\mathbf{R}$ is large (noisy measurement), $\\mathbf{K}_k$ is small ‚Üí trust prediction more.\n",
    "\n",
    "### 5.3.2 State Correction\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{x}}_k &= \\hat{\\mathbf{x}}_k^{-} + \\mathbf{K}_k (\\mathbf{z}_k - \\mathbf{H} \\hat{\\mathbf{x}}_k^{-}) \\\\\n",
    "\\mathbf{P}_k &= (\\mathbf{I} - \\mathbf{K}_k \\mathbf{H}) \\mathbf{P}_k^{-} (\\mathbf{I} - \\mathbf{K}_k \\mathbf{H})^T + \\mathbf{K}_k \\mathbf{R} \\mathbf{K}_k^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The second equation uses the **Joseph form** for numerical stability, ensuring $\\mathbf{P}_k$ remains positive definite.\n",
    "\n",
    "## 5.4 Asynchronous Measurement Handling\n",
    "\n",
    "The camera does not always provide measurements (e.g., when the ArUco marker is occluded or the robot moves too fast causing motion blur). The EKF handles this gracefully:\n",
    "\n",
    "- **Every control cycle (10 Hz):** Run prediction step using odometry\n",
    "- **When camera data arrives:** Run update step to correct the estimate\n",
    "- **When camera unavailable:** Skip update step; rely purely on odometry (uncertainty $\\mathbf{P}$ grows)\n",
    "\n",
    "This architecture allows the filter to continue operating even with intermittent measurements.\n",
    "\n",
    "## 5.5 Kidnap Detection and Recovery\n",
    "\n",
    "The original EKF implementation included a **kidnap detector** based on position jumps:\n",
    "\n",
    "$$\n",
    "\\text{if } \\|\\mathbf{z}_k^{xy} - \\hat{\\mathbf{x}}_k^{-,xy}\\| > 30 \\text{ cm} \\implies \\text{Reject measurement (kidnap)}\n",
    "$$\n",
    "\n",
    "However, this was **disabled** in the final system in favor of a hardware-based approach using **ground sensors**. The proximity sensors on the robot's underside detect when the robot is lifted (reflected signal drops dramatically). When detected, the main control loop:\n",
    "\n",
    "1. Stops all motors\n",
    "2. Enters KIDNAPPED state\n",
    "3. Waits for robot to be placed back down (ground sensor signal returns)\n",
    "4. Waits 2 seconds for stabilization\n",
    "5. Requests a new camera measurement\n",
    "6. **Resets EKF** with new measurement as initial state: $\\hat{\\mathbf{x}}_0 = \\mathbf{z}_{\\text{camera}}$, $\\mathbf{P}_0 = \\mathbf{R}$\n",
    "7. Triggers path replanning from new location\n",
    "\n",
    "This approach is more reliable than jump detection since large position changes can occur legitimately when the camera briefly loses tracking.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Advantages of EKF:**\n",
    "- **Sensor Fusion:** Combines high-rate odometry with accurate but sparse camera measurements\n",
    "- **Uncertainty Quantification:** Provides not just a position estimate but also confidence bounds\n",
    "- **Predictive Capability:** Can estimate position even when measurements are unavailable\n",
    "- **Optimal (for linear systems):** Minimizes mean squared error when assumptions hold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e97d1",
   "metadata": {},
   "source": [
    "## üî¨ Filtering (EKF) [To modify bc I didnt read the code enough yet]\n",
    "\n",
    "This module provides a smooth and accurate estimate of the robot's pose by fusing the noisy odometry model with the noisy vision measurements.\n",
    "\n",
    "### Theory: Extended Kalman Filter (EKF)\n",
    "\n",
    "The EKF is necessary because the robot's motion model (kinematics) and the vision measurements are both subject to noise. Furthermore, the robot's motion is governed by non-linear equations (due to the $\\sin(\\theta)$ and $\\cos(\\theta)$ terms), requiring the use of the Extended version of the Kalman Filter.\n",
    "\n",
    "The EKF proceeds in two steps per cycle:\n",
    "\n",
    "1.  **Prediction (Time Update):** Uses the robot's kinematic model ($\\text{Thymio.state\\_extrapolation\\_f}$) to project the current pose ($\\hat{x}_{k-1}$) and covariance ($\\hat{P}_{k-1}$) forward in time, accounting for the process noise ($Q$).\n",
    "2.  **Update (Measurement Update):** Corrects the predicted pose ($\\bar{x}_k$) using the measurement ($\\text{z}_k$) from the vision system, weighted by the Kalman Gain ($K$).\n",
    "\n",
    "### Kinematic Model and Noise ($\\text{thymio\\_math\\_model.py}$)\n",
    "\n",
    "The $\\text{Thymio}$ class defines the differential-drive kinematic equations and computes the Process Noise Covariance matrix ($Q$).\n",
    "\n",
    "* **Process Noise ($Q$):** $Q$ is calculated dynamically using the Jacobian ($G$) of the state transition function with respect to the input (wheel speeds, $v_l, v_r$).\n",
    "    $$Q = G \\Sigma_u G^T + Q_{\\text{model}}$$\n",
    "    The input covariance $\\Sigma_u$ accounts for noise proportional to the wheel speed: $\\text{var}(v) \\propto |u|$, which is essential for realistic noise modeling in small robots.\n",
    "* **Parameters:** The constants $L$ ($9.5 \\text{ cm}$ axle length), $R$ ($2.2 \\text{ cm}$ wheel radius), and the noise scaling factors ($\\text{kvl} = 0.47, \\text{kvr} = 0.51$) were obtained through physical measurements and empirical data analysis of the Thymio.\n",
    "\n",
    "#### Code: EKF Initialization\n",
    "\n",
    "```python\n",
    "# CODE FROM state_estimation.py and thymio_math_model.py\n",
    "import numpy as np\n",
    "from utils import Pose\n",
    "# ... (rest of state_estimation.py and thymio_math_model.py content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43513e3d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Motion Control Module Implementation\n",
    "\n",
    "The robot's motion control is executed by the ThymioController class, which operates as a **Finite State Machine (FSM)**. This structure allows for seamless and stable arbitration between global, goal-oriented driving and local, reactive obstacle avoidance.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. FSM and Control Strategy: Blending and Stability\n",
    "\n",
    "#### Theory: FSM with Time-Based Control Blending\n",
    "\n",
    "The core design principle is the use of a time-based blending mechanism when transitioning from the AVOIDING state back to NAVIGATING. This technique is a critical engineering choice to prevent control signal instability (overshooting or oscillation) upon re-entry to the complex proportional goal-seeking behavior.\n",
    "\n",
    "The transition is governed by a fixed-duration cooldown phase (AVOIDANCE_DURATION = $\\mathbf{4.0\\text{ s}}$). During this time, the output motor commands are a linear interpolation between the stable, simple command (Drive Straight) and the complex navigation command (Move to Point).\n",
    "\n",
    "The blending factor, $\\text{decay}$, ensures a smooth ramp-down of the straight-drive influence:\n",
    "$$\\text{Command} = \\text{decay} \\cdot \\text{Drive}_{\\text{Straight}} + (1.0 - \\text{decay}) \\cdot \\text{Command}_{\\text{Navigate}}$$\n",
    "where $\\text{decay} \\to 1.0$ at the start of the cooldown and $\\text{decay} \\to 0.0$ at the end.\n",
    "\n",
    "#### Choices and Justification\n",
    "\n",
    "* Fixed Cooldown Duration ($\\mathbf{4.0\\text{ s}}$): This fixed time, rather than a proportional distance or angular correction, was chosen for its robustness in the constrained Thymio environment. It guarantees sufficient time for the robot to clear the immediate disturbance and for the external pose estimation system to provide settled, stable data.\n",
    "* Avoidance Threshold ($\\mathbf{400}$): This low $\\text{SENSOR\\_THRESHOLD}$ was selected to trigger avoidance preemptively (at a greater distance). Early reaction is preferred to aggressive, late braking, especially given the high $\\text{AVOID\\_SPEED}$ ($\\mathbf{125.0}$).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Goal-Oriented Driving: $\\text{\\_move\\_to\\_point()}$\n",
    "\n",
    "This module forms the core of the NAVIGATING state, calculating the motor commands required to eliminate the angular error relative to the next global waypoint.\n",
    "\n",
    "#### Theory: P-Controller with Regulated Linear Speed\n",
    "\n",
    "The controller uses a Proportional (P) Controller for angular control, coupled with a regulated linear speed function based on the $\\cos$ of the heading error.\n",
    "\n",
    "* Angular Control ($\\omega$): $\\omega$ is proportional to the $\\text{heading\\_error}$ ($\\omega = \\text{KP\\_ROT} \\times \\text{heading\\_error}$).\n",
    "* Linear Speed ($V$): $V$ is attenuated by $\\max(0.0, \\cos(\\text{heading\\_error}))$. This design ensures the robot slows down when it needs to make large turning corrections (i.e., when $\\text{heading\\_error}$ approaches $\\pm 90^\\circ$), resulting in a stable, efficient turn-in-place maneuver instead of a large arcing path.\n",
    "\n",
    "#### Parameter Tuning\n",
    "\n",
    "| Parameter | Value | Rationale for Choice |\n",
    "| :--- | :--- | :--- |\n",
    "| $\\text{KP\\_ROT}$ | $\\mathbf{120.0}$ | A high proportional gain, empirically tuned for quick, decisive steering with minimal oscillation around the target heading. |\n",
    "| $\\text{MAX\\_SPEED}$ | $\\mathbf{125.0}$ | Set high to ensure high path efficiency when driving straight, matching the $\\text{AVOID\\_SPEED}$. |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Reactive Avoidance: $\\text{\\_avoid\\_obstacles()}$\n",
    "\n",
    "This function generates the motor commands during the AVOIDING state, taking over path following to prevent collision with a local obstacle.\n",
    "\n",
    "#### Theory: Differential Steering with Dynamic Weighting\n",
    "\n",
    "The avoidance mechanism is a differential steering technique where the $\\text{turn}_{\\text{weight}}$ is derived from the weighted sensor inputs.\n",
    "\n",
    "A key design innovation is the use of dynamic sensor weighting based on the symmetry of the frontal threat:\n",
    "\n",
    "$$\\text{turn}_{\\text{weight}} = (S_0 \\cdot W_{\\text{outer}} + S_1 \\cdot W_{\\text{inner}}) - (S_3 \\cdot W_{\\text{inner}} + S_4 \\cdot W_{\\text{outer}})$$\n",
    "\n",
    "If the inner sensors ($\\text{sensor}[1]$ and $\\text{sensor}[3]$) are roughly equal ($\\text{symmetric threat}$), the controller forces a dramatic increase in the weight of the outer sensors ($W_{\\text{outer}} \\to 2.3$, $W_{\\text{inner}} \\to 0$), compelling the robot to commit quickly to the side with the slightest opening. Otherwise, standard weights ($\\mathbf{1.7}$ inner, $\\mathbf{1.0}$ outer) are used for finer correctional steering.\n",
    "\n",
    "#### Parameter Tuning\n",
    "\n",
    "| Parameter | Value | Rationale for Choice |\n",
    "| :--- | :--- | :--- |\n",
    "| $\\text{AVOID\\_SPEED}$ | $\\mathbf{125.0}$ | The avoidance speed is intentionally set equal to $\\text{MAX\\_SPEED}$. This decision prioritizes clearing the obstacle area quickly, relying on the aggressive $\\text{angular\\_speed}$ calculation to manage the turn rather than slowing down. |\n",
    "| $\\text{Rotation Gain}$ | $\\mathbf{0.025}$ | A low gain applied to the $\\text{turn}_{\\text{weight}}$. This is necessary because the $\\text{turn}_{\\text{weight}}$ can be a very large number (sum of sensor readings). The low gain scales the powerful sensor input into a stable, non-spinning rotational impulse. |\n",
    "\n",
    "## Code\n",
    "\n",
    "```python\n",
    "# CODE FROM control.py (Modified for independent execution)\n",
    "import math\n",
    "from utils import Point, Pose, angle_between, normalize_angle, RobotState\n",
    "import time\n",
    "\n",
    "class ThymioController:\n",
    "    def __init__(self):\n",
    "        self.KP_ROT = 120.0  # P-gain for rotation\n",
    "        self.MAX_SPEED = 125.0  # Thymio units\n",
    "        self.AVOID_SPEED = 125.0\n",
    "        self.SENSOR_THRESHOLD = 400  # Threshold to trigger avoidance\n",
    "        self.AVOIDANCE_DURATION = 4.0  # Number of seconds to stay in avoidance state\n",
    "        self.FRONT_SENSOR_MIN_DELTA = 30\n",
    "        self.last_avoidance_time = 0.0\n",
    "\n",
    "        # Internal State\n",
    "        self.state: RobotState = RobotState.NAVIGATING\n",
    "\n",
    "    def update(self, current_pose: Pose, target_pos: Point, sensor_data: list[int]) -> tuple[float, float]:\n",
    "        \"\"\"\n",
    "        FSM and transition logic with time-based blending.\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "\n",
    "        max_front_sensor = max(sensor_data[0:5])\n",
    "        is_obstacle_present = max_front_sensor > self.SENSOR_THRESHOLD\n",
    "\n",
    "        nav_l, nav_r = self._move_to_point(current_pose, target_pos)\n",
    "\n",
    "        if is_obstacle_present:\n",
    "            # State 1: IMMEDIATE AVOIDANCE\n",
    "            self.state = RobotState.AVOIDING\n",
    "            self.last_avoidance_time = current_time\n",
    "            return self._avoid_obstacles(sensor_data)\n",
    "\n",
    "        elif self.state == RobotState.AVOIDING:\n",
    "            # State 2: TIME-BASED COOLDOWN AND BLENDING\n",
    "            elapsed = current_time - self.last_avoidance_time\n",
    "            if elapsed < self.AVOIDANCE_DURATION:\n",
    "                \n",
    "                # Calculate decay factor (1.0 at start of cooldown, 0.0 at end)\n",
    "                decay = max(0.0, 1.0 - (elapsed / self.AVOIDANCE_DURATION))\n",
    "\n",
    "                # Drive straight component (target of the blend)\n",
    "                straight_l = self.MAX_SPEED\n",
    "                straight_r = self.MAX_SPEED\n",
    "\n",
    "                # Blend controls: decay (straight) + (1.0 - decay) (navigation)\n",
    "                l_cmd = decay * straight_l + (1.0 - decay) * nav_l\n",
    "                r_cmd = decay * straight_r + (1.0 - decay) * nav_r\n",
    "\n",
    "                return int(l_cmd), int(r_cmd)\n",
    "            else:\n",
    "                # Cooldown finished\n",
    "                self.state = RobotState.NAVIGATING\n",
    "\n",
    "        # Default: Navigation\n",
    "        return nav_l, nav_r\n",
    "\n",
    "    def _move_to_point(self, current_pose: Pose, target_pos: Point) -> tuple[float, float]:\n",
    "        \"\"\"\n",
    "        P-Controlled navigation to a target point.\n",
    "        \"\"\"\n",
    "        angle_to_target = angle_between(current_pose, target_pos)\n",
    "        heading_error = normalize_angle(angle_to_target - current_pose.theta)\n",
    "        angular_speed = self.KP_ROT * heading_error\n",
    "        linear_speed = self.MAX_SPEED * max(0.0, math.cos(heading_error))\n",
    "\n",
    "        l_speed = linear_speed - angular_speed\n",
    "        r_speed = linear_speed + angular_speed\n",
    "\n",
    "        return int(l_speed), int(r_speed)\n",
    "\n",
    "    def _avoid_obstacles(self, sensor_data: list[int]) -> tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Differential steering for evasive maneuvers with dynamic sensor weighting.\n",
    "        \"\"\"\n",
    "        \n",
    "        inner_sensor_weight = 1.7\n",
    "        outer_sensor_weight = 1.0\n",
    "        \n",
    "        # Dynamic Weighting Logic: if frontal threat is symmetric, prioritize outer sensors\n",
    "        if abs(sensor_data[1] - sensor_data[3]) < self.FRONT_SENSOR_MIN_DELTA:\n",
    "            inner_sensor_weight = 0.0\n",
    "            outer_sensor_weight = 2.3\n",
    "\n",
    "        turn_weight = (\n",
    "            sensor_data[0] * outer_sensor_weight\n",
    "            + sensor_data[1] * inner_sensor_weight\n",
    "            - sensor_data[3] * inner_sensor_weight\n",
    "            - sensor_data[4] * outer_sensor_weight\n",
    "        )\n",
    "\n",
    "        angular_speed = turn_weight * 0.025\n",
    "\n",
    "        l_speed = self.AVOID_SPEED + angular_speed\n",
    "        r_speed = self.AVOID_SPEED - angular_speed\n",
    "\n",
    "        return int(l_speed), int(r_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fffcb7",
   "metadata": {},
   "source": [
    "## üåé Global Navigation\n",
    "\n",
    "This module is responsible for defining the static environment and calculating the shortest, obstacle-free path for the robot's navigation (using $\\text{pathfinding.py}$ and map data from $\\text{vision.py}$).\n",
    "\n",
    "### Theory: Visibility Graph and A\\* Algorithm\n",
    "\n",
    "The global path planning strategy uses a **Visibility Graph (VG)** combined with the **A\\* search algorithm**.\n",
    "\n",
    "* **Visibility Graph Construction:** The VG's nodes consist of all detected obstacle vertices (after C-Space buffering) plus the robot's starting position and the final goal position. An edge exists between any two nodes if the straight-line segment connecting them is visible.\n",
    "* **A\\* Search:** A\\* is an **informed search algorithm** used to traverse the VG. It finds the path minimizing the total estimated cost, $f(n)$, where:\n",
    "    $$\n",
    "    f(n) = g(n) + h(n)\n",
    "    $$\n",
    "    $g(n)$ is the actual distance traveled, and $h(n)$ is the Euclidean distance heuristic to the goal. A\\* guarantees finding the globally shortest path on the graph.\n",
    "\n",
    "### Implementation Details ($\\text{vision.py}$ and $\\text{pathfinding.py}$)\n",
    "\n",
    "* **Obstacle Configuration Space (C-Space):** A critical step in $\\text{vision.py}$ is buffering all static obstacles by a margin ($\\text{ROBOT\\_RADIUS\\_CM} + \\text{padding}$) to ensure the resulting path avoids collision with the robot's physical body.\n",
    "* **Graph Augmentation ($\\text{add\\_robot\\_to\\_graph}$):** Since the robot's start position changes, the $\\text{base\\_graph}$ (obstacles + goal) is dynamically augmented by adding the robot's current EKF-filtered $\\text{Pose}$ as the temporary start node, ensuring the path is always computed from the robot's latest location.\n",
    "\n",
    "#### Code: Pathfinding Module\n",
    "\n",
    "```python\n",
    "# CODE FROM pathfinding.py (Used to execute the module independently)\n",
    "import heapq\n",
    "import networkx as nx\n",
    "from utils import Point, HeuristicFunction, euclidean_distance\n",
    "\n",
    "def reconstruct_path(came_from: dict[int, int], current: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Backtracks from a goal node (ID) to start node (ID) to generate the final path list.\n",
    "    \"\"\"\n",
    "    total_path = [current]\n",
    "    while current in came_from:\n",
    "        current = came_from[current]\n",
    "        total_path.append(current)\n",
    "    return total_path[::-1]\n",
    "\n",
    "def find_path(\n",
    "    graph: nx.Graph,\n",
    "    start_node: int,\n",
    "    goal_node: int,\n",
    "    heuristic: HeuristicFunction = euclidean_distance,\n",
    ") -> list[int]:\n",
    "    # ... (Full implementation of A* algorithm using heapq for priority queue management)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e706b90",
   "metadata": {},
   "source": [
    "## üß≠ Local Navigation: Reactive Obstacle Avoidance\n",
    "\n",
    "This module provides an **immediate, priority control signal** whenever an obstacle is detected, overriding the global goal-seeking behavior of the navigation system. The local navigation mechanism is structured into an **Active Evasion Phase** and a **Blended Cooldown Phase**.\n",
    "\n",
    "---\n",
    "\n",
    "### Theory: Differential Steering and Control Blending\n",
    "\n",
    "The system transitions between two distinct control modes:\n",
    "\n",
    "1.  **Active Evasion:** This mode is triggered immediately when proximity sensor readings exceed the threshold ($\\text{SENSOR\\_THRESHOLD} = \\mathbf{400}$). The robot calculates a rotational impulse based on the threat's proximity, and the goal is to induce a rapid pivot away from the side experiencing the highest sensor threat.\n",
    "\n",
    "2.  **Blended Cooldown:** Upon clearing the obstacle, the robot enters a **time-based decay phase** of $\\mathbf{4.0\\text{ seconds}}$ ($\\text{AVOIDANCE\\_DURATION}$). This mechanism serves two purposes: preventing the robot from steering back into the obstacle's path and ensuring a smooth, non-oscillatory re-entry to the navigation path by avoiding a sudden oversteer back towards the target.\n",
    "\n",
    "#### Evasion Drive Calculation\n",
    "\n",
    "The core of the avoidance mechanism is **differential steering**, where the $\\text{turn}_{\\text{weight}}$ is derived from the threat imbalance and applied symmetrically to the base speed ($\\text{AVOID}_{\\text{SPEED}}$):\n",
    "\n",
    "$$\n",
    "L_{\\text{speed}} = \\text{AVOID}_{\\text{SPEED}} + \\text{turn}_{\\text{weight}} \\times \\text{gain}\n",
    "$$\n",
    "$$\n",
    "R_{\\text{speed}} = \\text{AVOID}_{\\text{SPEED}} - \\text{turn}_{\\text{weight}} \\times \\text{gain}\n",
    "$$\n",
    "\n",
    "A **positive** $\\text{turn}_{\\text{weight}}$ (indicating an obstacle on the left) results in $L_{\\text{speed}}$ increasing and $R_{\\text{speed}}$ decreasing, forcing a turn **right**. The opposite action occurs for a negative $\\text{turn}_{\\text{weight}}$.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "* **Turning Weight Calculation (Dynamic):** The $\\text{turn}_{\\text{weight}}$ is calculated using a **dynamic sensor weighting** strategy to adapt to different obstacle geometries. Base weights ($\\mathbf{1.7}$ inner, $\\mathbf{1.0}$ outer) are used for general threats. However, if the threat is frontal/symmetric ($\\text{sensor}[1] \\approx \\text{sensor}[3]$), inner sensors are ignored ($\\mathbf{0.0}$) and outer sensors are heavily prioritized ($\\mathbf{2.3}$) to force a sharp directional evasion.\n",
    "\n",
    "    The weighted difference is computed as:\n",
    "    $$\n",
    "    \\text{turn}_{\\text{weight}} = (S_0 \\cdot W_{\\text{outer}} + S_1 \\cdot W_{\\text{inner}}) - (S_3 \\cdot W_{\\text{inner}} + S_4 \\cdot W_{\\text{outer}})\n",
    "    $$\n",
    "\n",
    "* **Blend Logic:** During the $\\mathbf{4.0\\text{ s}}$ cooldown, the blend factor ($\\text{decay}$) interpolates the motor commands:\n",
    "\n",
    "    $$\\text{Command} = \\text{decay} \\cdot \\text{Straight} + (1.0 - \\text{decay}) \\cdot \\text{Navigate}$$\n",
    "\n",
    "    This ensures the robot gradually shifts from prioritizing simple forward stability (when $\\text{decay} \\approx 1.0$) back to complex goal-seeking control (when $\\text{decay} \\approx 0.0$).\n",
    "\n",
    "# Code:\n",
    "\n",
    "```python\n",
    "# CODE FROM control.py (Demonstrates avoidance and blending logic)\n",
    "# This code is executed in the update loop by the FSM.\n",
    "import time\n",
    "# ... (imports and class definition)\n",
    "\n",
    "class ThymioController:\n",
    "    # ... (relevant __init__ variables: AVOIDANCE_DURATION, SENSOR_THRESHOLD, etc.)\n",
    "\n",
    "    def update(self, current_pose: Pose, target_pos: Point, sensor_data: list[int]) -> tuple[float, float]:\n",
    "        # ... (transition logic, including time-based elif self.state == RobotState.AVOIDING block)\n",
    "        \n",
    "        # When blending:\n",
    "        # decay = max(0.0, 1.0 - (elapsed / self.AVOIDANCE_DURATION))\n",
    "        # l_cmd = decay * straight_l + (1.0 - decay) * nav_l\n",
    "        # r_cmd = decay * straight_r + (1.0 - decay) * nav_r\n",
    "        # ...\n",
    "\n",
    "    def _avoid_obstacles(self, sensor_data: list[int]) -> tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Differential steering for evasive maneuvers with dynamic sensor weighting.\n",
    "        \"\"\"\n",
    "        # ... (dynamic weighting calculation using inner_sensor_weight, outer_sensor_weight)\n",
    "        \n",
    "        # turn_weight calculation:\n",
    "        # turn_weight = (S0 * W_outer + S1 * W_inner - S3 * W_inner - S4 * W_outer)\n",
    "\n",
    "        # angular_speed = turn_weight * 0.025\n",
    "\n",
    "        # l_speed = self.AVOID_SPEED + angular_speed\n",
    "        # r_speed = self.AVOID_SPEED - angular_speed\n",
    "\n",
    "        # return int(l_speed), int(r_speed)\n",
    "        pass # Placeholder for full code execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aab538",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

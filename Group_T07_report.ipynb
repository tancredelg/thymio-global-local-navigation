{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3633b89",
   "metadata": {},
   "source": [
    "# Basics of Mobile Robotics, Project report \n",
    "\n",
    "The final project for EPFL's [**MICRO-452: Basics of Mobile Robotics**](https://edu.epfl.ch/coursebook/en/basics-of-mobile-robotics-MICRO-452) course. This project implements a complete navigation stack for a differential drive robot, featuring computer vision, graph-based path planning, sensor fusion (EKF), and reactive control.\n",
    "\n",
    "**The Concept:** Inspired by the Disney movie WALL·E, the robot EVE must navigate through the interior of the spaceship Axiom - a complex and hazardous environment. Her mission is to reach, her love,  WALL·E as fast as possible, while avoiding a series of dangerous obstacles.\n",
    "The map includes:\n",
    "\n",
    "- Windows: Represented as static dark-blue polygonal regions that EVE must not cross. These areas act as boundaries or fatal zones, similar to voids in the spacecraft’s structure.\n",
    "\n",
    "- Humans (“Passengers”): Large, drifting individuals who move unpredictably through the corridors. They function as dynamic obstacles that EVE must detect and avoid in real time.\n",
    "\n",
    "![Example environment](images/warped.jpg)\n",
    "\n",
    "\n",
    "## Group members & tasks repartition\n",
    "- Tancrede Lamort de Gail: Vision, global navigation, motion control\n",
    "- John Constantin: Vision, global navigation, motion control\n",
    "- Marcus Edjolo: Local naviation, global navigation, motion control\n",
    "- Yvan Barragan: Filtering, motion control\n",
    "\n",
    "\n",
    "## 1. Physical Setup\n",
    "\n",
    "The project tasks a Thymio II robot with navigating from a start pose to a goal pose while avoiding two distinct types of obstacles.\n",
    "\n",
    "### 1.1 The Environment\n",
    "\n",
    "The playground consist of a white rectangle of 130x92 cm delimitated by four Aruco Marker place on each of the 4 corners.\n",
    "\n",
    "The environment presents a dual-layer challenge:\n",
    "\n",
    "1.  **Static \"Global\" Obstacles (The Windows):**\n",
    "    -   **Physicality:** Flat, dark blue polygonal cutouts.\n",
    "    -   **Visibility:** _Invisible_ to the robot's onboard horizontal proximity sensors, but clearly visible to the overhead global camera.\n",
    "    -   **Constraint:** The robot cannot drive over them. Avoiding them relies entirely on accurate global localization and path planning.\n",
    "2.  **Ephemeral \"Local\" Obstacles (The Passengers):**\n",
    "    -   **Physicality:** 3D physical objects (cylinders, blocks) roughly the size of the robot.\n",
    "    -   **Visibility:** Visible to the robot's onboard sensors, but _ignored_ by the global camera mapping (or placed after mapping is complete).\n",
    "    -   **Constraint:** These act as dynamic blockers. The robot must use local sensing to reactively avoid them without losing track of its global objective.\n",
    "\n",
    "### 1.2 The Robot\n",
    "\n",
    "The **Thymio II** is a differential-drive mobile robot.\n",
    "\n",
    "-   **Sensors:** 5 front-facing and 2 rear-facing horizontal IR proximity sensors (for local avoidance).\n",
    "-   **Comms:** Python API interfacing via USB/RF dongle.\n",
    "-   **Markers:** An ArUco marker is mounted on top of the robot to facilitate high-precision global tracking (Pose: $x, y, \\theta$) via the overhead camera.\n",
    "\n",
    "### 1.3 The Camera\n",
    "\n",
    "We're usging our smartphone mounted overhead to have a better quality and beging less sentistive to the environment. With the software \"Camo studio\" we are able the read the phone camera at $1920 \\times 1080$, RGB, 30 FPS and transmit to the program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5361a",
   "metadata": {},
   "source": [
    "## 2. Vision System\n",
    "The vision systeme serves as the \"GPS\" of the system, responsible for:\n",
    "\n",
    "1.  **Initial Mapping:** Detecting static obstacle polygons and the goal area.\n",
    "2.  **Live Tracking:** Providing absolute position estimates to correct the robot's drifting odometry.\n",
    "\n",
    "\n",
    "### 2.1 System Architecture\n",
    "\n",
    "The vision pipeline operates in two distinct phases:\n",
    "\n",
    "#### 2.1.1 Initialization Phase (Static Mapping)\n",
    "During startup, the system captures a single reference frame and extracts the complete static environment:\n",
    "- Calibration markers define the workspace boundaries\n",
    "- Static obstacles (the \"spacecraft windows\") are segmented and converted to polygonal representations\n",
    "- Goal location (WALL·E's position) is identified\n",
    "- Visibility graph is pre-computed for path planning\n",
    "\n",
    "#### 2.1.2 Runtime Phase (Real-Time Localization)\n",
    "During navigation, the system continuously:\n",
    "- Captures frames at approximately 10 Hz\n",
    "- Detects (EVE's position) the robot's ArUco marker\n",
    "- Computes the robot's pose $(x, y, \\theta)$\n",
    "- Provides measurements to the Extended Kalman Filter for sensor fusion\n",
    "\n",
    "### 2.2 Coordinate Systems and Perspective Transformation\n",
    "\n",
    "The vision system must bridge two distinct coordinate frames:\n",
    "\n",
    "**Camera Frame:** The raw image captured by the overhead camera, measured in pixels with origin at top-left, where coordinates are $(u, v) \\in [0, 1920] \\times [0, 1080]$.\n",
    "\n",
    "**World Frame:** The robot's navigation space, measured in centimeters with origin at bottom-left of the workspace, where coordinates are $(x, y) \\in [0,132.5 ] \\times [0, 92.5]$.\n",
    "\n",
    "\n",
    "  <figure style=\"margin: 0; text-align: center;\">\n",
    "    <img src=\"images/warmup_frame.jpg\" alt=\"Raw BGR image\" width=\"500\"/>\n",
    "    <figcaption>Raw BGR image</figcaption>\n",
    "  </figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0413feb",
   "metadata": {},
   "source": [
    "#### 2.2.1 Perspective Correction\n",
    "\n",
    "Due to the camera's mounting angle and lens distortion, the raw image exhibits perspective distortion. We use a **homography transformation** to warp the image into an orthographic (bird's-eye) view:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} u' \\\\ v' \\\\ w \\end{bmatrix} = \\mathbf{H} \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{H} \\in \\mathbb{R}^{3 \\times 3}$ is the perspective transformation matrix computed via OpenCV's `getPerspectiveTransform()` using the four corner calibration markers as control points.\n",
    "\n",
    "The final pixel coordinates after perspective correction are:\n",
    "$$\n",
    "x_{\\text{px}} = \\frac{u'}{w}, \\quad y_{\\text{px}} = \\frac{v'}{w}\n",
    "$$\n",
    "\n",
    "#### 2.2.2 Metric Conversion\n",
    "\n",
    "After warping, we convert from the rectified pixel space to world coordinates (cm):\n",
    "\n",
    "$$\n",
    "x_{\\text{cm}} = \\frac{x_{\\text{px}}}{\\alpha_x}, \\quad y_{\\text{cm}} = H_{\\text{map}} - \\frac{y_{\\text{px}}}{\\alpha_y}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha_x = \\frac{W_{\\text{px}}}{W_{\\text{cm}}}$ and $\\alpha_y = \\frac{H_{\\text{px}}}{H_{\\text{cm}}}$ are pixel-to-centimeter scaling factors\n",
    "- $H_{\\text{map}}$ is the map height (flipping y-axis from image convention)\n",
    "\n",
    "\n",
    "  <figure style=\"margin: 0; text-align: center;\">\n",
    "    <img src=\"images/warped.jpg\" alt=\"Warped image\" width=\"500\"/>\n",
    "    <figcaption>Warped image</figcaption>\n",
    "  </figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d50ef9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 2.3 Environment Segmentation\n",
    "\n",
    "#### 2.3.1 Color-Based Segmentation\n",
    "We segment obstacles and the goal using **HSV color space** thresholding (finding experimentally), which is more robust to lighting variations than RGB:\n",
    "\n",
    "**Static Obstacles (Dark Blue):**\n",
    "$$\n",
    "\\text{Mask}_{\\text{obstacle}} = \\{(h,s,v) \\mid 90° \\leq h \\leq 165°, \\, 40 \\leq s \\leq 255, \\, 80 \\leq v \\leq 255\\}\n",
    "$$\n",
    "\n",
    "**Goal Region (Red):**\n",
    "Due to red's wraparound in HSV (0° and 360° are both red), we use a dual-range mask:\n",
    "$$\n",
    "\\text{Mask}_{\\text{goal}} = \\text{Mask}_{\\text{red1}} \\cup \\text{Mask}_{\\text{red2}}\n",
    "$$\n",
    "where:\n",
    "- $\\text{Mask}_{\\text{red1}}: 0° \\leq h \\leq 15°$\n",
    "- $\\text{Mask}_{\\text{red2}}: 165° \\leq h \\leq 180°$\n",
    "\n",
    "\n",
    "#### 2.3.2 Morphological Filtering\n",
    "To eliminate noise and fill gaps in the segmented regions, we apply morphological operations with a $7 \\times 7$ kernel:\n",
    "1. **Opening** (erosion → dilation): Removes small noise blobs\n",
    "2. **Closing** (dilation → erosion): Fills internal holes in obstacles\n",
    "\n",
    "<div style=\"display: flex; gap: 20px; justify-content: center; align-items: flex-start;\">\n",
    "\n",
    "  <figure style=\"margin: 0; text-align: center;\">\n",
    "    <img src=\"images/obstacle_mask.jpg\" alt=\"Obstacle mask\" width=\"300\"/>\n",
    "    <figcaption>Obstacle mask</figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"margin: 0; text-align: center;\">\n",
    "    <img src=\"images/target_mask.jpg\" alt=\"Target mask\" width=\"300\"/>\n",
    "    <figcaption>Target mask</figcaption>\n",
    "  </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f890058",
   "metadata": {},
   "source": [
    "### 2.5 Configuration Space Construction\n",
    "\n",
    "#### 2.5.1 Polygon Approximation\n",
    "\n",
    "Raw contours from segmentation are noisy and contain hundreds of vertices. We simplify them into approximated polygons via cv2's `approxPolyDP` function, which uses the **Douglas-Peucker algorithm** with an error proportional to contour length:\n",
    "\n",
    "$$\n",
    "\\epsilon = 0.015 \\cdot \\text{arcLength}(\\text{contour}) \\\\\n",
    "P = \\text{cv2.approxPolyDP}(C, \\epsilon)\n",
    "$$\n",
    "\n",
    "This produces polygons with 4-12 vertices while preserving essential shape features.\n",
    "\n",
    "#### 2.5.2 C-Space Inflation\n",
    "To account for the robot's physical dimensions, we **buffer** each obstacle polygon by the robot's radius plus a safety margin:\n",
    "\n",
    "$$\n",
    "\\mathcal{O}_{\\text{buffered}} = \\mathcal{O} \\oplus B_r\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{O}$ is the original obstacle polygon\n",
    "- $B_r$ is a disk of radius $r = r_{\\text{robot}} + \\text{padding}$ (≈ 5.5 cm)\n",
    "- $\\oplus$ denotes Minkowski sum (implemented via Shapely's `buffer()` with mitre join style, and mitre chopping to avoid excessively pointy edges)\n",
    "\n",
    "This operation effectively reduces the robot to a point in configuration space, simplifying collision checking.\n",
    "\n",
    "#### 2.5.3 Vertex Merging\n",
    "The buffering operation can introduce redundant vertices on long straight edges. To reduce graph complexity, we merge vertices closer than a threshold $d_{\\text{min}} = 6.0$ cm:\n",
    "\n",
    "$$\n",
    "\\text{if } \\|v_i - v_{i+1}\\| < d_{\\text{min}}, \\quad v_{\\text{merged}} = \\frac{v_i + v_{i+1}}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3167dcd",
   "metadata": {},
   "source": [
    "### 2.6 Visibility Graph Construction\n",
    "\n",
    "The visibility graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ encodes all collision-free straight-line paths through the environment. We leverage two Python libraries for this task: **Shapely** for geometric operations (polygon intersection, line-polygon collision detection) and **NetworkX** for graph representation and manipulation.\n",
    "\n",
    "#### 2.6.1 Node Addition\n",
    "We begin by constructing the node set $\\mathcal{V}$, which includes all vertices from the buffered obstacle polygons plus the goal location:\n",
    "\n",
    "$$\n",
    "\\mathcal{V} = \\mathcal{V}_{\\text{obstacles}} \\cup \\{v_{\\text{goal}}\\}\n",
    "$$\n",
    "\n",
    "Each node $v_i$ is added to a NetworkX graph with an attribute `pos` storing its $(x, y)$ coordinates in centimeters.\n",
    "\n",
    "#### 2.6.2 Obstacle Edge Insertion\n",
    "Before computing visibility between arbitrary node pairs, we first add all **obstacle boundary edges** to the graph. For each obstacle polygon with vertices $\\{v_0, v_1, \\ldots, v_{n-1}\\}$, we connect consecutive vertices:\n",
    "\n",
    "$$\n",
    "\\mathcal{E}_{\\text{obstacle}} = \\{(v_i, v_{i+1 \\mod n}) \\mid i = 0, \\ldots, n-1\\}\n",
    "$$\n",
    "\n",
    "Each edge is weighted by Euclidean distance:\n",
    "$$\n",
    "w(v_i, v_j) = \\|v_i - v_j\\|_2 = \\sqrt{(x_j - x_i)^2 + (y_j - y_i)^2}\n",
    "$$\n",
    "\n",
    "This ensures that paths can traverse obstacle perimeters when necessary.\n",
    "\n",
    "#### 2.6.3 Visibility Edge Computation\n",
    "Next, we test visibility between all pairs of nodes. Two nodes $v_i, v_j$ are connected by an edge if:\n",
    "1. They are not already connected (to avoid duplicate obstacle edges)\n",
    "2. The line segment $\\overline{v_i v_j}$ does not intersect any obstacle interior\n",
    "\n",
    "We implement visibility checking using Shapely's geometric predicates. The test verifies that the line segment neither crosses through nor lies entirely within any obstacle:\n",
    "\n",
    "$$\n",
    "\\text{visible}(v_i, v_j) = \\begin{cases}\n",
    "\\text{True} & \\text{if } \\overline{v_i v_j} \\cap \\mathcal{O}_k = \\emptyset \\, \\forall k \\text{ and } \\overline{v_i v_j} \\not\\subset \\mathcal{O}_k \\, \\forall k \\\\\n",
    "\\text{False} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The second condition is critical for concave obstacles - a line segment could lie entirely within a U-shaped obstacle without technically \"intersecting\" its boundary.\n",
    "\n",
    "**Shrinking Heuristic:** To avoid false positives from floating-point precision issues at polygon vertices, we test a **slightly shortened** version of each line segment:\n",
    "\n",
    "$$\n",
    "\\overline{v_i v_j}_{\\text{test}} = \\{v_i + 0.001 \\cdot (v_j - v_i) \\to v_i + 0.999 \\cdot (v_j - v_i)\\}\n",
    "$$\n",
    "\n",
    "This prevents spurious intersections when paths graze obstacle corners.\n",
    "\n",
    "#### 2.6.4 Boundary Cropping\n",
    "After constructing the full graph, we perform a final validation step: **remove any nodes that lie outside the map boundaries**. Due to the buffering operation, some obstacle vertices may extend beyond the physical workspace limits $[0, W_{\\text{map}}] \\times [0, H_{\\text{map}}]$.\n",
    "\n",
    "We identify and remove all such nodes:\n",
    "$$\n",
    "\\mathcal{V}_{\\text{invalid}} = \\{v_i \\in \\mathcal{V} \\mid x_i < 0 \\text{ or } x_i > W_{\\text{map}} \\text{ or } y_i < 0 \\text{ or } y_i > H_{\\text{map}}\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{V}_{\\text{final}} = \\mathcal{V} \\setminus \\mathcal{V}_{\\text{invalid}}\n",
    "$$\n",
    "\n",
    "NetworkX automatically removes all edges connected to deleted nodes, ensuring graph integrity.\n",
    "\n",
    "#### 2.6.5 Base Graph Storage\n",
    "The result of this process is the **base visibility graph**, which contains only obstacle vertices and the goal node. This graph is computed once during initialization and stored as `self.base_graph`. The robot's start position is intentionally excluded at this stage to enable dynamic replanning after kidnapping events.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.7 Dynamic Graph Augmentation\n",
    "\n",
    "Since the robot's start position changes after each kidnapping event, the visibility graph must be **dynamically updated**:\n",
    "\n",
    "1. **Base Graph:** Contains only obstacle vertices and the goal (precomputed once)\n",
    "2. **Runtime Augmentation:** When the robot is relocated:\n",
    "   - Add a new start node $v_{\\text{start}}$ at the robot's current pose\n",
    "   - Test visibility from $v_{\\text{start}}$ to all existing nodes\n",
    "   - Connect $v_{\\text{start}}$ to all visible nodes with weighted edges\n",
    "   - Run A* search on the augmented graph\n",
    "\n",
    "This strategy avoids recomputing the entire $O(n^3)$ visibility graph (where $n \\approx 40$ nodes) after every kidnapping - we only perform $O(n^2)$ visibility tests from the new start node.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9763e",
   "metadata": {},
   "source": [
    "### 2.8 ArUco Marker Detection and Pose Estimation\n",
    "\n",
    "#### 2.8.1 Pose Computation\n",
    "Given the four corners of the detected marker $\\{C_0, C_1, C_2, C_3\\}$ (ordered as Top-Left, Top-Right, Bottom-Right, Bottom-Left), we compute:\n",
    "\n",
    "**Position** (centroid):\n",
    "$$\n",
    "x = \\frac{1}{4}\\sum_{i=0}^{3} C_{i,x}, \\quad y = \\frac{1}{4}\\sum_{i=0}^{3} C_{i,y}\n",
    "$$\n",
    "\n",
    "**Orientation** (from marker's left edge):\n",
    "$$\n",
    "\\theta = \\text{atan2}(C_{0,y} - C_{3,y}, \\, C_{0,x} - C_{3,x})\n",
    "$$\n",
    "\n",
    "This orientation vector points along the robot's forward direction, as the marker is mounted with its left edge aligned with the robot's heading.\n",
    "\n",
    "### 2.8 Measurement Uncertainty\n",
    "\n",
    "The camera provides pose estimates $(x, y, \\theta)$ with empirically measured covariance:\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_{\\text{camera}} = \\begin{bmatrix}\n",
    "0.00119 & 0 & 0 \\\\\n",
    "0 & 0.00179 & 0 \\\\\n",
    "0 & 0 & 0.000061\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This translates to standard deviations of approximately:\n",
    "- Position: $\\sigma_x \\approx 3.4$ mm, $\\sigma_y \\approx 4.2$ mm\n",
    "- Orientation: $\\sigma_\\theta \\approx 0.45°$\n",
    "\n",
    "These values were obtained through repeated measurements of a stationary robot and used to tune the Extended Kalman Filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829cbd5",
   "metadata": {},
   "source": [
    "## 3. Path Planning\n",
    "\n",
    "Once the vision system has constructed a visibility graph of the environment, the robot needs to compute the shortest collision-free path from its current position to the goal. We implement this using the **A\\* (A-star) search algorithm**.\n",
    "\n",
    "\n",
    "### 3.1 Implementation Details\n",
    "\n",
    "The implementation uses a **priority queue** (min-heap) to efficiently retrieve the node with the lowest $f$-cost at each iteration:\n",
    "\n",
    "**NetworkX Integration:** Our visibility graph is stored as a `networkx.Graph` object where:\n",
    "- Nodes have attribute `pos: Point` (world coordinates)\n",
    "- Edges have attribute `weight: float` (Euclidean distance)\n",
    "\n",
    "This abstraction allows A\\* to operate independently of the graph construction method.\n",
    "\n",
    "### 3.2 Waypoint Generation\n",
    "\n",
    "The output of A\\* is a sequence of node indices:\n",
    "$$\n",
    "\\text{path} = [n_{\\text{start}}, n_1, n_2, \\ldots, n_{k-1}, n_{\\text{goal}}]\n",
    "$$\n",
    "\n",
    "We convert these to world-space waypoints by extracting the `pos` attribute:\n",
    "$$\n",
    "\\text{waypoints} = [(x_0, y_0), (x_1, y_1), \\ldots, (x_k, y_k)]\n",
    "$$\n",
    "\n",
    "The controller then navigates sequentially through these waypoints (nodes on green line in the figure below), moving to the next one once within a threshold distance of the current target.\n",
    "\n",
    "  <figure style=\"margin: 0; text-align: center;\">\n",
    "    <img src=\"images\\visu.jpg\" alt=\"Optimal path\" width=\"500\"/>\n",
    "    <figcaption>Optimal path</figcaption>\n",
    "  </figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b618c5",
   "metadata": {},
   "source": [
    "# 4. Local Navigation and Control\n",
    "\n",
    "While the vision system and path planner provide a global roadmap (Section 1), the robot must execute this plan in the real world using onboard sensors and actuators. This layer handles two critical tasks: (1) **waypoint tracking**—following the planned path, and (2) **reactive obstacle avoidance**—responding to dynamic obstacles not captured during mapping.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Control Architecture (Finite State Machine)\n",
    "\n",
    "The control system operates as a predictable **hierarchical state machine** with two primary operational states. The transition logic manages the stability requirements of both global path-following and local evasion.\n",
    "\n",
    "### 4.1.1 State: NAVIGATING\n",
    "\n",
    "This state encompasses both goal-seeking behavior (tracking waypoints) and the smooth return phase after avoidance.\n",
    "\n",
    "**Objective:** Drive toward the current waypoint or smoothly blend control to return to waypoint tracking.\n",
    "\n",
    "#### A. Waypoint Tracking (Pure Navigation)\n",
    "\n",
    "This control law is used when no avoidance maneuver is in progress ($\\alpha=0$).\n",
    "\n",
    "**Control Law:** Proportional control on heading error with speed regulation based on alignment:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_{\\text{error}} &= \\text{normalize}\\left(\\text{atan2}(y_{\\text{target}} - y, x_{\\text{target}} - x) - \\theta\\right) \\\\\n",
    "\\omega &= K_p \\cdot \\theta_{\\text{error}} \\\\\n",
    "v &= v_{\\max} \\cdot \\max(0, \\cos(\\theta_{\\text{error}}))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $K_p = 120$ is the proportional gain for angular velocity\n",
    "* $v_{\\max} = 125$ Thymio units\n",
    "* $\\text{normalize}(\\cdot)$ wraps angles to $[-\\pi, \\pi]$\n",
    "\n",
    "The cosine term elegantly handles the speed-accuracy tradeoff: when the robot is well-aligned with the target ($\\theta_{\\text{error}} \\approx 0$), it drives at full speed. When misaligned ($|\\theta_{\\text{error}}| \\approx \\pi/2$), it stops forward motion and rotates in place.\n",
    "\n",
    "**Differential Drive Kinematics:**\n",
    "$$\n",
    "\\begin{align}\n",
    "v_L &= v - \\omega \\\\\n",
    "v_R &= v + \\omega\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### B. Decay Phase (Smooth Return)\n",
    "\n",
    "This control law is used for duration $T = 4.0$ seconds after obstacle clearance ($\\alpha > 0$). It achieves a smooth transition back to pure navigation control.\n",
    "\n",
    "**Control Law (Linear Blending):**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha(t) &= \\max\\left(0, 1 - \\frac{t}{T}\\right) \\quad \\text{(decay factor)} \\\\\n",
    "v_L &= \\alpha \\cdot v_{\\max} + (1 - \\alpha) \\cdot v_{L,\\text{nav}} \\\\\n",
    "v_R &= \\alpha \\cdot v_{\\max} + (1 - \\alpha) \\cdot v_{R,\\text{nav}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $v_{L/R,\\text{nav}}$ are the wheel speeds output by the Waypoint Tracking controller. This linear interpolation prevents oscillations and ensures the robot smoothly reorients toward the path.\n",
    "\n",
    "### 4.1.2 State: AVOIDING\n",
    "\n",
    "**Trigger:** Any front proximity sensor exceeds threshold ($\\mathbf{400}$ units).\n",
    "\n",
    "**Objective:** Steer away from obstacle using a computationally trivial reactive controller.\n",
    "\n",
    "**Control Law (Differential Steering):**\n",
    "\n",
    "$$\n",
    "\\tau = w_{\\text{outer}}(s_0 - s_4) + w_{\\text{inner}}(s_1 - s_3)\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $s_i$ are the five front proximity sensor readings (indexed left to right).\n",
    "- $w_{\\text{outer}} = 1.0$, $w_{\\text{inner}} = 1.7$ are tuned weights\n",
    "* $\\tau$ is the turn bias (positive = turn right, negative = turn left).\n",
    "\n",
    "The motor commands are:\n",
    "$$\n",
    "\\begin{align}\n",
    "v_L &= v_{\\text{avoid}} + k \\cdot \\tau \\\\\n",
    "v_R &= v_{\\text{avoid}} - k \\cdot \\tau\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with gain $k = 0.025$.\n",
    "\n",
    "**Adaptive Weighting:** If the left-right sensor difference is too small ($|s_1 - s_3| < 30$), indicating a symmetric frontal obstacle, the system adapts by setting $w_{\\text{inner}} = 0$ and increasing $w_{\\text{outer}} = 2.3$. This forces the robot to commit to a sharp turn, preventing it from getting stuck on a head-on collision.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 Waypoint Advancement and Design Rationale\n",
    "\n",
    "### Waypoint Advancement\n",
    "\n",
    "The robot monitors its Euclidean distance to the current waypoint:\n",
    "$$\n",
    "d = \\sqrt{(x_{\\text{waypoint}} - x)^2 + (y_{\\text{waypoint}} - y)^2}\n",
    "$$\n",
    "\n",
    "The waypoint is considered reached, and the controller advances to the next one in the sequence, when $d < 1.5$ cm.\n",
    "\n",
    "### Design Rationale\n",
    "\n",
    "* **Why FSM?** Finite state machines provide predictable, debuggable behavior. By fusing the decay phase into the NAVIGATING state, we emphasize that the ultimate objective remains goal-oriented navigation, regardless of the intermediate control law.\n",
    "* **Why Decay Phase?** Instantaneous switching from obstacle avoidance back to navigation causes \"chattering\" (the robot oscillates between states when near an obstacle). The decay phase acts as a **low-pass filter** on the control commands, smoothly restoring the proportional navigation signal.\n",
    "* **Why Braitenberg-inspired Differential Steering?** While not globally optimal, this control law is computationally trivial (required for quick reflexes on the Thymio) and provides robust local evasion, effectively complementing the high-level global path planning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678fef5",
   "metadata": {},
   "source": [
    "## 5. Extended Kalman Filter (EKF) formulation for pose estimation\n",
    "\n",
    "### 5.1 Principle of the Extended Kalman Filter\n",
    "\n",
    "The Extended Kalman Filter (EKF) is a state estimator for **non-linear** dynamical systems. It assumes the system can be written as:\n",
    "\n",
    "* **Nonlinear state (process) model:**\n",
    "    $$\\mathbf{x}_{k+1} = f(\\mathbf{x}_k, \\mathbf{u}_k) + \\mathbf{w}_k$$\n",
    "\n",
    "* **Nonlinear measurement model:**\n",
    "    $$\\mathbf{z}_k = h(\\mathbf{x}_k) + \\mathbf{v}_k$$\n",
    "\n",
    "with:\n",
    "* $\\mathbf{x}_k$: state vector at time step $k$,\n",
    "* $\\mathbf{u}_k$: known control input (wheel speeds in our case),\n",
    "* $\\mathbf{z}_k$: measurement vector (camera-based pose),\n",
    "* $\\mathbf{w}_k \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q}_k)$: process noise,\n",
    "* $\\mathbf{v}_k \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R}_k)$: measurement noise.\n",
    "\n",
    "Following Becker’s notation in *Kalman Filter from the Ground Up*, the EKF linearizes the non-linear functions $f(\\cdot)$ and $h(\\cdot)$ around the current estimate using their Jacobians, and then applies the standard Kalman prediction–update loop to fuse odometry and camera measurements.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Nonlinear state-space model of the Thymio robot\n",
    "\n",
    "We use a **unicycle-like kinematic model** with state:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_k =\n",
    "\\begin{bmatrix}\n",
    "x_k \\\\\n",
    "y_k \\\\\n",
    "\\theta_k\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $x_k, y_k$ are the robot position in the global (camera) frame [cm],\n",
    "* $\\theta_k$ is the robot heading [rad].\n",
    "\n",
    "The input is the pair of wheel linear velocities derived from Thymio’s internal velocity units:\n",
    "\n",
    "$$\n",
    "\\mathbf{u}_k =\n",
    "\\begin{bmatrix}\n",
    "v_{L,k} \\\\\n",
    "v_{R,k}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Given the wheelbase $L$, the linear and angular velocities are:\n",
    "\n",
    "$$\n",
    "V_k = \\frac{v_{L,k} + v_{R,k}}{2}, \\qquad\n",
    "\\omega_k = \\frac{v_{R,k} - v_{L,k}}{L}\n",
    "$$\n",
    "\n",
    "With sampling period $\\Delta t$, the discrete-time kinematics are:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_{k+1} &= x_k + V_k \\cos(\\theta_k)\\,\\Delta t, \\\\\n",
    "y_{k+1} &= y_k + V_k \\sin(\\theta_k)\\,\\Delta t, \\\\\n",
    "\\theta_{k+1} &= \\theta_k + \\omega_k\\,\\Delta t.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This defines the nonlinear state transition function:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{k+1} = f(\\mathbf{x}_k, \\mathbf{u}_k) =\n",
    "\\begin{bmatrix}\n",
    "x_k + V_k \\cos(\\theta_k)\\,\\Delta t \\\\\n",
    "y_k + V_k \\sin(\\theta_k)\\,\\Delta t \\\\\n",
    "\\theta_k + \\omega_k\\,\\Delta t\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is implemented as the `state_extrapolation_f` model of the Thymio robot.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 EKF prediction step\n",
    "\n",
    "#### 5.3.1 Linearization of the process model\n",
    "\n",
    "To propagate the state uncertainty, the EKF uses the Jacobian of $f$ with respect to the state:\n",
    "\n",
    "$$\n",
    "\\mathbf{F}_k = \\left.\\frac{\\partial f}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}=\\hat{\\mathbf{x}}_{k|k},\\, \\mathbf{u}=\\mathbf{u}_k}\n",
    "$$\n",
    "\n",
    "From the kinematic model, using $V_k$ as defined above, we obtain:\n",
    "\n",
    "$$\n",
    "\\mathbf{F}_k =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -V_k \\sin(\\hat{\\theta}_{k|k})\\,\\Delta t \\\\\n",
    "0 & 1 & \\;\\;\\;V_k \\cos(\\hat{\\theta}_{k|k})\\,\\Delta t \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which matches the `jacobian_dF` function in the implementation.\n",
    "\n",
    "To inject **input noise** into the state covariance, we also use the Jacobian of $f$ with respect to the input:\n",
    "\n",
    "$$\n",
    "\\mathbf{G}_k = \\left.\\frac{\\partial f}{\\partial \\mathbf{u}}\\right|_{\\mathbf{x}=\\hat{\\mathbf{x}}_{k|k},\\, \\mathbf{u}=\\mathbf{u}_k}\n",
    "$$\n",
    "\n",
    "The chosen input mapping leads to:\n",
    "\n",
    "$$\n",
    "\\mathbf{G}_k =\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{2}\\Delta t \\cos(\\hat{\\theta}_{k|k}) & \\frac{1}{2}\\Delta t \\cos(\\hat{\\theta}_{k|k}) \\\\\n",
    "\\frac{1}{2}\\Delta t \\sin(\\hat{\\theta}_{k|k}) & \\frac{1}{2}\\Delta t \\sin(\\hat{\\theta}_{k|k}) \\\\\n",
    "-\\frac{\\Delta t}{L} & \\frac{\\Delta t}{L}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which corresponds to the `jacobian_G` function.\n",
    "\n",
    "#### 5.3.2 State and covariance prediction\n",
    "\n",
    "The prediction step is defined as:\n",
    "\n",
    "* **State extrapolation:**\n",
    "    $$\\hat{\\mathbf{x}}_{k+1|k} = f(\\hat{\\mathbf{x}}_{k|k}, \\mathbf{u}_k)$$\n",
    "\n",
    "* **Covariance extrapolation:**\n",
    "    $$\\mathbf{P}_{k+1|k} = \\mathbf{F}_k\\,\\mathbf{P}_{k|k}\\,\\mathbf{F}_k^\\top + \\mathbf{Q}_k$$\n",
    "\n",
    "where $\\mathbf{Q}_k$ is the process noise covariance. In our design, $\\mathbf{Q}_k$ is dominated by the input noise (wheel speed uncertainty), with a smaller additional term capturing modeling errors (see Section 5.5).\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 EKF update step — full pose measurement\n",
    "\n",
    "The overhead vision system provides a direct measurement of the robot pose:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_k =\n",
    "\\begin{bmatrix}\n",
    "x^{\\text{cam}}_k \\\\\n",
    "y^{\\text{cam}}_k \\\\\n",
    "\\theta^{\\text{cam}}_k\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\mathbf{x}_k + \\mathbf{v}_k\n",
    "$$\n",
    "\n",
    "with measurement noise $\\mathbf{v}_k$.\n",
    "\n",
    "This corresponds to a **linear measurement model**:\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x}_k) = \\mathbf{x}_k, \\qquad\n",
    "\\mathbf{z}_k = h(\\mathbf{x}_k) + \\mathbf{v}_k\n",
    "$$\n",
    "\n",
    "so the observation matrix is simply:\n",
    "\n",
    "$$\n",
    "\\mathbf{H}_k = \\frac{\\partial h}{\\partial \\mathbf{x}} = \\mathbf{I}_3\n",
    "$$\n",
    "\n",
    "The update step is then:\n",
    "\n",
    "* **Innovation:**\n",
    "    $$\\mathbf{y}_k = \\mathbf{z}_k - h(\\hat{\\mathbf{x}}_{k|k-1}) = \\mathbf{z}_k - \\hat{\\mathbf{x}}_{k|k-1}$$\n",
    "\n",
    "* **Innovation covariance:**\n",
    "    $$\\mathbf{S}_k = \\mathbf{H}_k \\mathbf{P}_{k|k-1} \\mathbf{H}_k^\\top + \\mathbf{R}_k$$\n",
    "\n",
    "* **Kalman gain:**\n",
    "    $$\\mathbf{K}_k = \\mathbf{P}_{k|k-1} \\mathbf{H}_k^\\top \\mathbf{S}_k^{-1}$$\n",
    "\n",
    "* **State update:**\n",
    "    $$\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1} + \\mathbf{K}_k \\mathbf{y}_k$$\n",
    "\n",
    "* **Covariance update (Joseph form):**\n",
    "    $$\\mathbf{P}_{k|k} = \\left(\\mathbf{I} - \\mathbf{K}_k \\mathbf{H}_k\\right) \\mathbf{P}_{k|k-1} \\left(\\mathbf{I} - \\mathbf{K}_k \\mathbf{H}_k\\right)^\\top + \\mathbf{K}_k \\mathbf{R}_k \\mathbf{K}_k^\\top$$\n",
    "\n",
    "This form is numerically stable and keeps $\\mathbf{P}_{k|k}$ symmetric and positive semi-definite.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.5 Uncertainty modelling and calibration\n",
    "\n",
    "All noise terms are modeled as zero-mean Gaussian random variables:\n",
    "\n",
    "* **Process noise:** $\\mathbf{w}_k \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{Q}_k)$\n",
    "* **Measurement noise:** $\\mathbf{v}_k \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R}_k)$\n",
    "\n",
    "The key design choice in our EKF is that both $\\mathbf{Q}_k$ and $\\mathbf{R}$ are **data-driven**:\n",
    "\n",
    "* $\\mathbf{Q}_k$ is mostly built from the experimentally measured variability of Thymio’s wheel speeds.\n",
    "* $\\mathbf{R}$ is obtained from the variability of the camera’s pose estimates.\n",
    "\n",
    "#### 5.5.1 Input noise and process noise $\\mathbf{Q}_k$\n",
    "\n",
    "We consider two contributions:\n",
    "1.  **Input-dependent noise** due to uncertain wheel speeds.\n",
    "2.  **Additional model noise** to capture unmodeled effects.\n",
    "\n",
    "##### (a) Experimental acquisition of wheel speed variances (Results in 5.8)\n",
    "\n",
    "To characterize the wheel speed noise, the Thymio was placed **in the air** so that the wheels could spin freely without interacting with the ground. For a series of target speeds (in Thymio units), we:\n",
    "\n",
    "1.  Commanded a constant target speed to the robot.\n",
    "2.  Recorded a set of speed samples from each motor.\n",
    "3.  Computed the sample variance of the measured speed for that target.\n",
    "\n",
    "This procedure was repeated for several target speeds. During analysis we observed:\n",
    "\n",
    "* A clear **positive correlation** between the magnitude of the command and the variance: higher speed commands produced larger variance in the measured speed.\n",
    "* Above approximately **200 Thymio units**, the variance increased very sharply and became dominated by outliers and nonlinearities, making it unsuitable for building a simple model.\n",
    "* We therefore **restricted the data used for modelling** to commands up to **120 Thymio units**, which we defined as the maximum operational speed for our application, and where the variance behaviour is still well-behaved and approximately linear.\n",
    "\n",
    "Using these filtered data (up to 120 units), we fitted a **linear regression model** for each wheel:\n",
    "\n",
    "$$\n",
    "\\operatorname{var}(u_L) \\approx a_L\\,|u_L| + b_L,\n",
    "\\qquad\n",
    "\\operatorname{var}(u_R) \\approx a_R\\,|u_R| + b_R\n",
    "$$\n",
    "\n",
    "where $u_L, u_R$ are the raw Thymio commands, and the parameters $a_L, a_R, b_L, b_R$ are obtained from the regression.\n",
    "\n",
    "In the implementation these relationships are encoded as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{var}(u_{L,k}) &= \\max\\big(k_{vL}\\,|u_{L,k}| + \\text{var}_{vL,\\text{base}},\\; \\text{var}_{vL,\\text{base}}\\big), \\\\\n",
    "\\operatorname{var}(u_{R,k}) &= \\max\\big(k_{vR}\\,|u_{R,k}| + \\text{var}_{vR,\\text{base}},\\; \\text{var}_{vR,\\text{base}}\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $k_{vL}, k_{vR}$ are slopes from the regression and $\\text{var}_{(\\cdot),\\text{base}}$ enforce a minimum variance level.\n",
    "\n",
    "These variances are then converted from Thymio units to physical units (cm/s) using the speed calibration factor $c$:\n",
    "\n",
    "$$\n",
    "\\operatorname{var}(v_{L,k}) = c^2\\,\\operatorname{var}(u_{L,k}), \\qquad\n",
    "\\operatorname{var}(v_{R,k}) = c^2\\,\\operatorname{var}(u_{R,k})\n",
    "$$\n",
    "\n",
    "which yields the **input noise covariance**:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}_{u,k} =\n",
    "\\begin{bmatrix}\n",
    "\\operatorname{var}(v_{L,k}) & 0 \\\\\n",
    "0 & \\operatorname{var}(v_{R,k})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To propagate this noise into the state, we use the previously defined Jacobian $\\mathbf{G}_k$:\n",
    "\n",
    "$$\n",
    "\\mathbf{Q}_{\\text{input},k} = \\mathbf{G}_k \\, \\boldsymbol{\\Sigma}_{u,k} \\, \\mathbf{G}_k^\\top\n",
    "$$\n",
    "\n",
    "This construction explicitly encodes the observed behaviour: as the wheel speeds increase (within the modeled range up to 120 units), their variance increases, and this higher input variance is correctly projected into larger uncertainty in $(x,y,\\theta)$.\n",
    "\n",
    "##### (b) Additional process noise\n",
    "\n",
    "On top of the input noise, we add a small **model noise** term to account for unmodelled dynamics (e.g., small slips, discretization errors):\n",
    "\n",
    "$$\n",
    "\\mathbf{Q}_{\\text{model}} =\n",
    "\\operatorname{diag}\n",
    "\\big(\n",
    "\\sigma_{x,\\text{model}}^2,\\;\n",
    "\\sigma_{y,\\text{model}}^2,\\;\n",
    "\\sigma_{\\theta,\\text{model}}^2\n",
    "\\big)\n",
    "$$\n",
    "\n",
    "The values $\\sigma_{x,\\text{model}}, \\sigma_{y,\\text{model}}, \\sigma_{\\theta,\\text{model}}$ were selected through a series of test runs, but **kept relatively small** on purpose. Since most of the motion uncertainty is already captured by the input noise model $\\mathbf{Q}_{\\text{input},k}$, this additional term only provides a modest baseline of process noise and avoids over-inflating the covariance.\n",
    "\n",
    "##### (c) Final process covariance\n",
    "\n",
    "The final process noise used in the EKF is:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathbf{Q}_k\n",
    "=\n",
    "\\mathbf{Q}_{\\text{input},k} + \\mathbf{Q}_{\\text{model}}\n",
    "}\n",
    "$$\n",
    "\n",
    "This design keeps $\\mathbf{Q}_k$ physically meaningful (driven by the measured Thymio behaviour) while still allowing some flexibility through the small model noise component.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.5.2 Measurement noise covariance $\\mathbf{R}$\n",
    "\n",
    "The measurement noise covariance $\\mathbf{R}$ was calibrated using the **camera pose detection system** itself.\n",
    "\n",
    "To do this, the robot was placed **completely still** at a fixed pose in the arena, and the overhead camera was used to record a set of pose measurements $\\{\\mathbf{z}_i\\}_{i=1}^N$, each:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_i =\n",
    "\\begin{bmatrix}\n",
    "x_{\\text{cam},i} \\\\\n",
    "y_{\\text{cam},i} \\\\\n",
    "\\theta_{\\text{cam},i}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "From these samples, we computed the sample mean:\n",
    "\n",
    "$$\\bar{\\mathbf{z}} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{z}_i$$\n",
    "\n",
    "and the **sample covariance**:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{R}} =\n",
    "\\frac{1}{N-1}\n",
    "\\sum_{i=1}^N\n",
    "(\\mathbf{z}_i - \\bar{\\mathbf{z}})\n",
    "(\\mathbf{z}_i - \\bar{\\mathbf{z}})^\\top\n",
    "$$\n",
    "\n",
    "In the EKF we then use a diagonal approximation of this covariance,\n",
    "\n",
    "$$\n",
    "\\mathbf{R} =\n",
    "\\begin{bmatrix}\n",
    "\\sigma_{x,\\text{cam}}^2 & 0 & 0 \\\\\n",
    "0 & \\sigma_{y,\\text{cam}}^2 & 0 \\\\\n",
    "0 & 0 & \\sigma_{\\theta,\\text{cam}}^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{R} =\n",
    "\\begin{bmatrix}\n",
    "1.186 \\times 10^{-3} & 0 & 0 \\\\\n",
    "0 & 1.787 \\times 10^{-3} & 0 \\\\\n",
    "0 & 0 & 6.101 \\times 10^{-5}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where the variances are taken from the diagonal (assuming no correlation between variables for simplicity of the scope of this project) of $\\hat{\\mathbf{R}}$ (after converting angles to radians). This way, the filter’s trust in the camera directly reflects the actual variability of the vision system.\n",
    "\n",
    "\n",
    "\n",
    "#### 5.5.3 Initial state and covariance\n",
    "\n",
    "Finally, the EKF must be initialized with:\n",
    "* $\\hat{\\mathbf{x}}_{0|0}$: an initial guess of the robot pose.\n",
    "* $\\mathbf{P}_{0|0}$: the initial estimation uncertainty.\n",
    "\n",
    "Consistent with standard practice, we use:\n",
    "* A reasonable initial pose estimate (e.g., from the first valid camera detection or a known start pose).\n",
    "* A **relatively large diagonal covariance** $\\mathbf{P}_{0|0}$ to express our initial lack of certainty and to allow the filter to adapt quickly to the first few measurements.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.6 Summary\n",
    "\n",
    "* The EKF is built on a **nonlinear Thymio kinematic model** for pose propagation and a **linear measurement model** (Option A: direct pose measurement from vision).\n",
    "* The EKF design focuses on **data-driven uncertainty modelling**:\n",
    "    * **Process Noise ($\\mathbf{Q}_k$):** Dominated by input noise, where wheel speed variances are measured experimentally (Thymio in the air) and propagated to the state via the Jacobian $\\mathbf{G}_k$.\n",
    "    * **Measurement Noise ($\\mathbf{R}$):** Obtained directly from the sample covariance of camera pose measurements while the robot is static.\n",
    "* This results in a filter whose parameters are grounded in the actual behaviour of the robot and the vision system, ensuring that the fusion weights (Kalman Gain) optimally balance the trust between odometry and vision.\n",
    "\n",
    "### 5.7 Implementation Overview (Pseudocode)\n",
    "\n",
    "To complement the EKF design and its mathematical formulation, this section provides a high-level pseudocode view of how the filter and the Thymio motion model are implemented in the project. The goal is not to present executable code, but to illustrate how the software structure corresponds directly to the mathematical equations in Section 5.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.7.1 Thymio Model: Motion and Process Noise\n",
    "\n",
    "The `ThymioModel` module encapsulates the nonlinear motion model, the calculation of Jacobians, and the generation of the data-driven process noise covariance matrix ($Q_k$) used by the EKF.\n",
    "\n",
    "```text\n",
    "CLASS ThymioModel\n",
    "\n",
    "    FUNCTION StateExtrapolation(x_state, u_input, dt)\n",
    "        // Implements the nonlinear kinematic model: x_{k+1} = f(x_k, u_k)\n",
    "        // INPUT:  x_state (3x1), u_input (wheel speeds), dt (timestep)\n",
    "        // OUTPUT: x_next (3x1)\n",
    "        BEGIN\n",
    "            V     ← (u_input.v_r + u_input.v_l) / 2\n",
    "            omega ← (u_input.v_r - u_input.v_l) / L\n",
    "            \n",
    "            x_next[0] ← x_state[0] + V * COS(x_state[2]) * dt\n",
    "            x_next[1] ← x_state[1] + V * SIN(x_state[2]) * dt\n",
    "            x_next[2] ← x_state[2] + omega * dt\n",
    "\n",
    "            RETURN x_next\n",
    "        END FUNCTION\n",
    "\n",
    "    FUNCTION JacobianDF(x_state, u_input, dt)\n",
    "        // Linearization w.r.t. the state: F_k = ∂f/∂x\n",
    "        BEGIN\n",
    "            V     ← (u_input.v_r + u_input.v_l) / 2\n",
    "            theta ← x_state[2]\n",
    "\n",
    "            F ← IdentityMatrix(3)\n",
    "            F[0, 2] ← -V * dt * SIN(theta)\n",
    "            F[1, 2] ←  V * dt * COS(theta)\n",
    "\n",
    "            RETURN F\n",
    "        END FUNCTION\n",
    "\n",
    "    FUNCTION JacobianG(x_state, dt)\n",
    "        // Linearization w.r.t. the inputs: G_k = ∂f/∂u\n",
    "        BEGIN\n",
    "            theta ← x_state[2]\n",
    "            \n",
    "            // Map wheel speed noise into state space (x, y, theta)\n",
    "            G ← Matrix(3, 2)\n",
    "            G[0, 0] ← 0.5 * dt * COS(theta);  G[0, 1] ← 0.5 * dt * COS(theta)\n",
    "            G[1, 0] ← 0.5 * dt * SIN(theta);  G[1, 1] ← 0.5 * dt * SIN(theta)\n",
    "            G[2, 0] ← -dt / L;                G[2, 1] ← dt / L\n",
    "\n",
    "            RETURN G\n",
    "        END FUNCTION\n",
    "\n",
    "    FUNCTION ComputeQ(x_state, u_input)\n",
    "        // Data-driven process noise model\n",
    "        BEGIN\n",
    "            // 1. Compute wheel-speed variances via regression model\n",
    "            var_vl ← MAX(k_vl * ABS(u_input.v_l) + var_base, var_base)\n",
    "            var_vr ← MAX(k_vr * ABS(u_input.v_r) + var_base, var_base)\n",
    "\n",
    "            // 2. Build input noise matrix\n",
    "            Sigma_u ← DiagonalMatrix(var_vl, var_vr)\n",
    "\n",
    "            // 3. Propagate through the dynamics\n",
    "            G ← JacobianG(x_state, dt)\n",
    "            Q_input ← G * Sigma_u * Transpose(G)\n",
    "\n",
    "            // 4. Add small diagonal model noise for stability\n",
    "            RETURN Q_input + Q_model_baseline\n",
    "        END FUNCTION\n",
    "\n",
    "END CLASS\n",
    "```\n",
    "\n",
    "### 5.7.2 Extended Kalman Filter Class\n",
    "\n",
    "The EKF class maintains the current state estimate, its error covariance, and the measurement noise parameters. It performs a prediction step at every control cycle and a correction step only when valid camera measurements are available.\n",
    "\n",
    "```text\n",
    "CLASS EKF\n",
    "\n",
    "    FUNCTION Initialize(model, x_init, P_init, R_matrix)\n",
    "        BEGIN\n",
    "            x ← x_init          // State estimate (3x1)\n",
    "            P ← P_init          // Covariance estimate (3x3)\n",
    "            R ← R_matrix        // Measurement noise (3x3)\n",
    "            sys_model ← model\n",
    "        END FUNCTION\n",
    "\n",
    "    FUNCTION PredictStep(u_input)\n",
    "        // Projects the state and covariance forward in time\n",
    "        BEGIN\n",
    "            // 1. Linearization and Noise Calculation\n",
    "            F ← sys_model.JacobianDF(x, u_input, dt)\n",
    "            Q ← sys_model.ComputeQ(x, u_input)\n",
    "\n",
    "            // 2. Nonlinear Prediction\n",
    "            x_pred ← sys_model.StateExtrapolation(x, u_input, dt)\n",
    "\n",
    "            // 3. Covariance Propagation\n",
    "            P_pred ← (F * P * Transpose(F)) + Q\n",
    "\n",
    "            // Update internal state\n",
    "            x ← x_pred\n",
    "            P ← P_pred\n",
    "        END FUNCTION\n",
    "\n",
    "    FUNCTION UpdateStep(z_meas)\n",
    "        // Corrects the predicted state using camera data\n",
    "        BEGIN\n",
    "            H ← IdentityMatrix(3)\n",
    "            z ← Vector(z_meas)\n",
    "\n",
    "            // 1. Compute Kalman Gain\n",
    "            // S = H P H^T + R\n",
    "            S ← (H * P * Transpose(H)) + R\n",
    "            K ← P * Transpose(H) * Inverse(S)\n",
    "\n",
    "            // 2. State Correction\n",
    "            // x = x + K(z - Hx)\n",
    "            innovation ← z - (H * x)\n",
    "            x_new ← x + (K * innovation)\n",
    "\n",
    "            // 3. Covariance Update (Joseph Form for numerical stability)\n",
    "            // P = (I - KH) P (I - KH)^T + K R K^T\n",
    "            I_KH ← (IdentityMatrix(3) - (K * H))\n",
    "            P_new ← (I_KH * P * Transpose(I_KH)) + (K * R * Transpose(K))\n",
    "\n",
    "            // Update internal state\n",
    "            x ← x_new\n",
    "            P ← P_new\n",
    "            \n",
    "            RETURN x, P\n",
    "        END FUNCTION\n",
    "\n",
    "END CLASS\n",
    "```\n",
    "### 5.7.3 EKF Cycle in the Main Control Loop\n",
    "\n",
    "The EKF runs synchronously within the main control loop. The prediction step is executed at every timestep (dt), while the update step is event-triggered.\n",
    "\n",
    "```text\n",
    "// Main Control Loop\n",
    "WHILE (SystemIsRunning) DO\n",
    "\n",
    "    // 1. Acquire Sensor Data\n",
    "    u_current.v_l ← GetMeasuredSpeedLeft()\n",
    "    u_current.v_r ← GetMeasuredSpeedRight()\n",
    "\n",
    "    // 2. EKF Prediction (Executed every cycle)\n",
    "    ekf_instance.PredictStep(u_current)\n",
    "\n",
    "    // 3. EKF Update (Conditional execution)\n",
    "    IF (NewCameraPoseAvailable()) THEN\n",
    "        z_cam ← GetCameraPose()\n",
    "        ekf_instance.UpdateStep(z_cam)\n",
    "    END IF\n",
    "\n",
    "    // 4. Usage\n",
    "    // Provide fused pose to navigation or path planning modules\n",
    "    current_pose ← ekf_instance.x\n",
    "    RunNavigationController(current_pose)\n",
    "\n",
    "END WHILE\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "### 5.8 Obtained Regression Model's for Each Wheel (quite not a good fit, but that's what thymio offers)\n",
    "The regression models for each wheel produced linear fits of the form $\\operatorname{var}(u) \\approx a|u| + b$. However, as expected for low-speed regions, the intercepts were negative (e.g., $-6.81$ for the left wheel and $-7.05$ for the right).\n",
    "\n",
    "Since variances must remain non-negative for physical and numerical consistency, a lower bound (**base variance**) was introduced based on the smallest experimentally observed variance for each wheel:\n",
    "\n",
    "* $\\text{var}_{L,\\text{base}} = 1.59$\n",
    "* $\\text{var}_{R,\\text{base}} = 1.24$\n",
    "\n",
    "During prediction, any value produced by the linear model that falls below these thresholds is replaced by the corresponding base variance. This **flooring mechanism** matches the behaviour implemented in `compute_Q()`, preventing the filter from collapsing at low speeds while retaining the essential trend encoded in the regression slopes ($0.289$ for the left wheel and $0.338$ for the right wheel), which capture how uncertainty grows with motor command.\n",
    "\n",
    "<div style=\"display: flex; gap: 20px; justify-content: center; align-items: flex-start;\">\n",
    "\n",
    "  <figure style=\"margin: 0; text-align: center;\">\n",
    "    <img src=\"images/speed_variance.jpg\" alt=\"speed_variance\" width=\"300\"/>\n",
    "    <figcaption>Speed variance left wheel</figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"margin: 0; text-align: center;\">\n",
    "    <img src=\"images/speed_varianceR.jpg\" alt=\"speed_variance\" width=\"300\"/>\n",
    "    <figcaption>Speed variance right wheel</figcaption>\n",
    "  </figure>\n",
    "</div>\n",
    "\n",
    "![Example environment](images/table.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad302cc",
   "metadata": {},
   "source": [
    "# Running the project\n",
    "\n",
    "To run the project (after `git clone` and `pip install -r requirements.txt`), simply open a terminal, navigate to the project's `src` directory, and execute the main script using Python 3:\n",
    "\n",
    "```bash\n",
    "cd src\n",
    "python main.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1053967",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This project successfully implemented a complete autonomous navigation stack for the Thymio II robot, integrating computer vision, graph-based planning, sensor fusion, and reactive control into a cohesive system capable of navigating complex environments with both static and dynamic obstacles.\n",
    "\n",
    "The **vision system** proved robust for environment mapping and real-time localization, with ArUco marker detection providing sub-centimeter accuracy. The visibility graph approach generated near-optimal paths through polygonal obstacle fields, while A* search efficiently computed collision-free trajectories. The **Extended Kalman Filter** effectively fused odometry and camera measurements, maintaining accurate pose estimates even during intermittent camera occlusions. The **hierarchical state machine controller** successfully balanced global path-following with local obstacle avoidance, demonstrating the value of combining deliberative planning with reactive behaviors.\n",
    "However, the system revealed several limitations. \n",
    "\n",
    "## Limitations and Edges cases\n",
    "\n",
    "### 1. Graph Node Obstruction\n",
    "Placing a local obstacle directly on a visibility graph node will block that node, potentially making the path impossible to traverse. The system has no mechanism to detect or replan around occupied nodes in real-time.\n",
    "\n",
    "### 2. Global Obstacle Collision During Local Avoidance\n",
    "The reactive avoidance controller operates independently of the global map. Depending on the obstacle's position relative to the path, the Braitenberg behavior may steer the robot into a global obstacle (window). The controller has no awareness of the static map during the AVOIDING state\n",
    "\n",
    "### 3. Initial Position Constraint\n",
    "The robot's starting position must be at least ROBOT_RADIUS + cspace_padding (≈5.5 cm) away from any obstacle. If placed too close, the buffered obstacle polygons may engulf the start node, causing path planning to fail.\n",
    "\n",
    "### 4. Goal Position in Obstacle\n",
    "If the goal (red heart) is detected inside or too close to a buffered obstacle polygon during mapping, the goal node will be created but may have no valid edges in the visibility graph, making it unreachable. The system doesn't validate goal accessibility during initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766efcf",
   "metadata": {},
   "source": [
    "## References and Bibliography\n",
    "\n",
    "Becker, G. (n.d.). *Kalman Filter from the Ground Up* [PDF]. Retrieved from https://kalmanfilter.net/book.html \n",
    "\n",
    "OpenCV ArUco Documentation: https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html\n",
    "\n",
    "NetworkX Documentation: https://networkx.org/\n",
    "\n",
    "Shapely Documentation: https://shapely.readthedocs.io/\n",
    "\n",
    "Thymio II Documentation: https://www.thymio.org/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f4b15",
   "metadata": {},
   "source": [
    "## Acknowledgments\n",
    "\n",
    "**Use of AI Tools:** Throughout this project, we utilized artificial intelligence assistants (ChatGPT, Gemini, Claude) as development tools for code debugging, algorithm explanation, documentation generation, and report writing assistance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aab538",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
